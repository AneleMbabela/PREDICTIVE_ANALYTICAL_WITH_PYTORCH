{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimpleRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import torch\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataÂ¶\n",
    "Source: https://www.kaggle.com/jemishdonda/headbrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age Range</th>\n",
       "      <th>Head Size(cm^3)</th>\n",
       "      <th>Brain Weight(grams)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4512</td>\n",
       "      <td>1530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3738</td>\n",
       "      <td>1297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4261</td>\n",
       "      <td>1335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3777</td>\n",
       "      <td>1282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4177</td>\n",
       "      <td>1590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3585</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3785</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3559</td>\n",
       "      <td>1255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3613</td>\n",
       "      <td>1355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3982</td>\n",
       "      <td>1375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender  Age Range  Head Size(cm^3)  Brain Weight(grams)\n",
       "0       1          1             4512                 1530\n",
       "1       1          1             3738                 1297\n",
       "2       1          1             4261                 1335\n",
       "3       1          1             3777                 1282\n",
       "4       1          1             4177                 1590\n",
       "5       1          1             3585                 1300\n",
       "6       1          1             3785                 1400\n",
       "7       1          1             3559                 1255\n",
       "8       1          1             3613                 1355\n",
       "9       1          1             3982                 1375"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_weight_data = pd.read_csv('datasets/headbrain.csv')\n",
    "\n",
    "size_weight_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237, 4)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_weight_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Head Size(cm^3)', 'Brain Weight(grams)'], dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_weight_data.drop(['Gender', 'Age Range'], axis=1, inplace=True)\n",
    "\n",
    "size_weight_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_weight_data = size_weight_data.rename(columns={'Head Size(cm^3)' : 'Head Size',\n",
    "                                                    'Brain Weight(grams)' : 'Brain Weight'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Head Size</th>\n",
       "      <th>Brain Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4512</td>\n",
       "      <td>1530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3738</td>\n",
       "      <td>1297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4261</td>\n",
       "      <td>1335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3777</td>\n",
       "      <td>1282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4177</td>\n",
       "      <td>1590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Head Size  Brain Weight\n",
       "0       4512          1530\n",
       "1       3738          1297\n",
       "2       4261          1335\n",
       "3       3777          1282\n",
       "4       4177          1590"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_weight_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.scatter(size_weight_data['Head Size'], size_weight_data['Brain Weight'], s=200)\n",
    "\n",
    "plt.xlabel('Head Size', fontsize=20)\n",
    "plt.ylabel('Brain Weight', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(size_weight_data[['Head Size']],\n",
    "                                                   size_weight_data[['Brain Weight']],\n",
    "                                                   test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((189, 1), (189, 1))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48, 1), (48, 1))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Head Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>189.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3622.841270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>361.195262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2720.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3381.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3609.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3858.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4747.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Head Size\n",
       "count   189.000000\n",
       "mean   3622.841270\n",
       "std     361.195262\n",
       "min    2720.000000\n",
       "25%    3381.000000\n",
       "50%    3609.000000\n",
       "75%    3858.000000\n",
       "max    4747.000000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardScaler_x = StandardScaler()\n",
    "\n",
    "standardScaler_x.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.890000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-9.780536e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.002656e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.506232e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.713366e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-3.842252e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.527863e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.120596e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "count  1.890000e+02\n",
       "mean  -9.780536e-17\n",
       "std    1.002656e+00\n",
       "min   -2.506232e+00\n",
       "25%   -6.713366e-01\n",
       "50%   -3.842252e-02\n",
       "75%    6.527863e-01\n",
       "max    3.120596e+00"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = pd.DataFrame(standardScaler_x.transform(x_train))\n",
    "\n",
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardScaler_y = StandardScaler()\n",
    "\n",
    "standardScaler_y.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.890000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.988906e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.002656e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.720822e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.348438e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-9.050276e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.750236e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.953039e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "count  1.890000e+02\n",
       "mean   7.988906e-16\n",
       "std    1.002656e+00\n",
       "min   -2.720822e+00\n",
       "25%   -6.348438e-01\n",
       "50%   -9.050276e-03\n",
       "75%    5.750236e-01\n",
       "max    2.953039e+00"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = pd.DataFrame(standardScaler_y.transform(y_train))\n",
    "\n",
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.152828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.059236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.106497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.637331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.151729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.774234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.273935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "count  48.000000\n",
       "mean    0.152828\n",
       "std     1.059236\n",
       "min    -2.106497\n",
       "25%    -0.637331\n",
       "50%     0.151729\n",
       "75%     0.774234\n",
       "max     2.273935"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = pd.DataFrame(standardScaler_x.transform(x_test))\n",
    "\n",
    "x_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardScaler_y = StandardScaler()\n",
    "\n",
    "standardScaler_y.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.890000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-3.935711e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.002656e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.720822e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.348438e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-9.050276e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.750236e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.953039e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "count  1.890000e+02\n",
       "mean  -3.935711e-17\n",
       "std    1.002656e+00\n",
       "min   -2.720822e+00\n",
       "25%   -6.348438e-01\n",
       "50%   -9.050276e-03\n",
       "75%    5.750236e-01\n",
       "max    2.953039e+00"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = pd.DataFrame(standardScaler_y.transform(y_train))\n",
    "\n",
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1289.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>122.041296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1012.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1213.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1280.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1380.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1590.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "count    48.000000\n",
       "mean   1289.916667\n",
       "std     122.041296\n",
       "min    1012.000000\n",
       "25%    1213.000000\n",
       "50%    1280.000000\n",
       "75%    1380.000000\n",
       "max    1590.000000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = pd.DataFrame(standardScaler_y.transform(y_test))\n",
    "\n",
    "y_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting values into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor(x_train.values.reshape(-1, 1), dtype = torch.float)\n",
    "y_train_tensor = torch.tensor(y_train.values.reshape(-1, 1), dtype = torch.float)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test.values.reshape(-1, 1), dtype = torch.float)\n",
    "y_test_tensor = torch.tensor(y_test.values.reshape(-1, 1), dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([189, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([189, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = 1\n",
    "hidden_layer = 12\n",
    "output_layer =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression using optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(torch.nn.Linear(input_layer, hidden_layer),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(hidden_layer, output_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration no: 0 and Loss: 174.4302978515625\n",
      "Iteration no: 1 and Loss: 174.3064422607422\n",
      "Iteration no: 2 and Loss: 174.1826629638672\n",
      "Iteration no: 3 and Loss: 174.05885314941406\n",
      "Iteration no: 4 and Loss: 173.93524169921875\n",
      "Iteration no: 5 and Loss: 173.8116912841797\n",
      "Iteration no: 6 and Loss: 173.68817138671875\n",
      "Iteration no: 7 and Loss: 173.5648193359375\n",
      "Iteration no: 8 and Loss: 173.44146728515625\n",
      "Iteration no: 9 and Loss: 173.31825256347656\n",
      "Iteration no: 10 and Loss: 173.19503784179688\n",
      "Iteration no: 11 and Loss: 173.07191467285156\n",
      "Iteration no: 12 and Loss: 172.94891357421875\n",
      "Iteration no: 13 and Loss: 172.82601928710938\n",
      "Iteration no: 14 and Loss: 172.70321655273438\n",
      "Iteration no: 15 and Loss: 172.5804443359375\n",
      "Iteration no: 16 and Loss: 172.45785522460938\n",
      "Iteration no: 17 and Loss: 172.33526611328125\n",
      "Iteration no: 18 and Loss: 172.2128448486328\n",
      "Iteration no: 19 and Loss: 172.0905303955078\n",
      "Iteration no: 20 and Loss: 171.96820068359375\n",
      "Iteration no: 21 and Loss: 171.84609985351562\n",
      "Iteration no: 22 and Loss: 171.72409057617188\n",
      "Iteration no: 23 and Loss: 171.6021728515625\n",
      "Iteration no: 24 and Loss: 171.48028564453125\n",
      "Iteration no: 25 and Loss: 171.35855102539062\n",
      "Iteration no: 26 and Loss: 171.2368621826172\n",
      "Iteration no: 27 and Loss: 171.11534118652344\n",
      "Iteration no: 28 and Loss: 170.99392700195312\n",
      "Iteration no: 29 and Loss: 170.87255859375\n",
      "Iteration no: 30 and Loss: 170.75135803222656\n",
      "Iteration no: 31 and Loss: 170.6302490234375\n",
      "Iteration no: 32 and Loss: 170.5091552734375\n",
      "Iteration no: 33 and Loss: 170.38829040527344\n",
      "Iteration no: 34 and Loss: 170.2675018310547\n",
      "Iteration no: 35 and Loss: 170.14678955078125\n",
      "Iteration no: 36 and Loss: 170.02621459960938\n",
      "Iteration no: 37 and Loss: 169.90573120117188\n",
      "Iteration no: 38 and Loss: 169.78533935546875\n",
      "Iteration no: 39 and Loss: 169.66510009765625\n",
      "Iteration no: 40 and Loss: 169.54490661621094\n",
      "Iteration no: 41 and Loss: 169.4248504638672\n",
      "Iteration no: 42 and Loss: 169.3048553466797\n",
      "Iteration no: 43 and Loss: 169.1849822998047\n",
      "Iteration no: 44 and Loss: 169.06529235839844\n",
      "Iteration no: 45 and Loss: 168.94564819335938\n",
      "Iteration no: 46 and Loss: 168.826171875\n",
      "Iteration no: 47 and Loss: 168.70672607421875\n",
      "Iteration no: 48 and Loss: 168.58737182617188\n",
      "Iteration no: 49 and Loss: 168.46820068359375\n",
      "Iteration no: 50 and Loss: 168.3491668701172\n",
      "Iteration no: 51 and Loss: 168.23028564453125\n",
      "Iteration no: 52 and Loss: 168.1116485595703\n",
      "Iteration no: 53 and Loss: 167.99307250976562\n",
      "Iteration no: 54 and Loss: 167.87466430664062\n",
      "Iteration no: 55 and Loss: 167.7563018798828\n",
      "Iteration no: 56 and Loss: 167.6381072998047\n",
      "Iteration no: 57 and Loss: 167.52001953125\n",
      "Iteration no: 58 and Loss: 167.4020538330078\n",
      "Iteration no: 59 and Loss: 167.28414916992188\n",
      "Iteration no: 60 and Loss: 167.1664276123047\n",
      "Iteration no: 61 and Loss: 167.04879760742188\n",
      "Iteration no: 62 and Loss: 166.93130493164062\n",
      "Iteration no: 63 and Loss: 166.81385803222656\n",
      "Iteration no: 64 and Loss: 166.6964874267578\n",
      "Iteration no: 65 and Loss: 166.5792999267578\n",
      "Iteration no: 66 and Loss: 166.46226501464844\n",
      "Iteration no: 67 and Loss: 166.3451690673828\n",
      "Iteration no: 68 and Loss: 166.228271484375\n",
      "Iteration no: 69 and Loss: 166.1115264892578\n",
      "Iteration no: 70 and Loss: 165.99481201171875\n",
      "Iteration no: 71 and Loss: 165.8782501220703\n",
      "Iteration no: 72 and Loss: 165.76173400878906\n",
      "Iteration no: 73 and Loss: 165.64532470703125\n",
      "Iteration no: 74 and Loss: 165.52908325195312\n",
      "Iteration no: 75 and Loss: 165.41294860839844\n",
      "Iteration no: 76 and Loss: 165.2968292236328\n",
      "Iteration no: 77 and Loss: 165.18089294433594\n",
      "Iteration no: 78 and Loss: 165.0651092529297\n",
      "Iteration no: 79 and Loss: 164.94932556152344\n",
      "Iteration no: 80 and Loss: 164.83358764648438\n",
      "Iteration no: 81 and Loss: 164.7180633544922\n",
      "Iteration no: 82 and Loss: 164.6026153564453\n",
      "Iteration no: 83 and Loss: 164.48727416992188\n",
      "Iteration no: 84 and Loss: 164.37205505371094\n",
      "Iteration no: 85 and Loss: 164.25694274902344\n",
      "Iteration no: 86 and Loss: 164.14186096191406\n",
      "Iteration no: 87 and Loss: 164.0269317626953\n",
      "Iteration no: 88 and Loss: 163.91209411621094\n",
      "Iteration no: 89 and Loss: 163.79733276367188\n",
      "Iteration no: 90 and Loss: 163.6826629638672\n",
      "Iteration no: 91 and Loss: 163.5681915283203\n",
      "Iteration no: 92 and Loss: 163.45370483398438\n",
      "Iteration no: 93 and Loss: 163.3393096923828\n",
      "Iteration no: 94 and Loss: 163.22506713867188\n",
      "Iteration no: 95 and Loss: 163.11093139648438\n",
      "Iteration no: 96 and Loss: 162.99685668945312\n",
      "Iteration no: 97 and Loss: 162.88287353515625\n",
      "Iteration no: 98 and Loss: 162.76904296875\n",
      "Iteration no: 99 and Loss: 162.65524291992188\n",
      "Iteration no: 100 and Loss: 162.5415496826172\n",
      "Iteration no: 101 and Loss: 162.42803955078125\n",
      "Iteration no: 102 and Loss: 162.31448364257812\n",
      "Iteration no: 103 and Loss: 162.20111083984375\n",
      "Iteration no: 104 and Loss: 162.08787536621094\n",
      "Iteration no: 105 and Loss: 161.9746856689453\n",
      "Iteration no: 106 and Loss: 161.861572265625\n",
      "Iteration no: 107 and Loss: 161.7485809326172\n",
      "Iteration no: 108 and Loss: 161.6356964111328\n",
      "Iteration no: 109 and Loss: 161.5229034423828\n",
      "Iteration no: 110 and Loss: 161.41018676757812\n",
      "Iteration no: 111 and Loss: 161.29757690429688\n",
      "Iteration no: 112 and Loss: 161.1851043701172\n",
      "Iteration no: 113 and Loss: 161.07260131835938\n",
      "Iteration no: 114 and Loss: 160.96034240722656\n",
      "Iteration no: 115 and Loss: 160.84805297851562\n",
      "Iteration no: 116 and Loss: 160.73599243164062\n",
      "Iteration no: 117 and Loss: 160.6239471435547\n",
      "Iteration no: 118 and Loss: 160.51199340820312\n",
      "Iteration no: 119 and Loss: 160.40013122558594\n",
      "Iteration no: 120 and Loss: 160.2883758544922\n",
      "Iteration no: 121 and Loss: 160.1767120361328\n",
      "Iteration no: 122 and Loss: 160.065185546875\n",
      "Iteration no: 123 and Loss: 159.95372009277344\n",
      "Iteration no: 124 and Loss: 159.84225463867188\n",
      "Iteration no: 125 and Loss: 159.73094177246094\n",
      "Iteration no: 126 and Loss: 159.6197052001953\n",
      "Iteration no: 127 and Loss: 159.5084991455078\n",
      "Iteration no: 128 and Loss: 159.39747619628906\n",
      "Iteration no: 129 and Loss: 159.28651428222656\n",
      "Iteration no: 130 and Loss: 159.17588806152344\n",
      "Iteration no: 131 and Loss: 159.06539916992188\n",
      "Iteration no: 132 and Loss: 158.95489501953125\n",
      "Iteration no: 133 and Loss: 158.8445281982422\n",
      "Iteration no: 134 and Loss: 158.73435974121094\n",
      "Iteration no: 135 and Loss: 158.62416076660156\n",
      "Iteration no: 136 and Loss: 158.51414489746094\n",
      "Iteration no: 137 and Loss: 158.40415954589844\n",
      "Iteration no: 138 and Loss: 158.2943115234375\n",
      "Iteration no: 139 and Loss: 158.1845245361328\n",
      "Iteration no: 140 and Loss: 158.07485961914062\n",
      "Iteration no: 141 and Loss: 157.9652099609375\n",
      "Iteration no: 142 and Loss: 157.85572814941406\n",
      "Iteration no: 143 and Loss: 157.74630737304688\n",
      "Iteration no: 144 and Loss: 157.63697814941406\n",
      "Iteration no: 145 and Loss: 157.5277099609375\n",
      "Iteration no: 146 and Loss: 157.4185333251953\n",
      "Iteration no: 147 and Loss: 157.30950927734375\n",
      "Iteration no: 148 and Loss: 157.20050048828125\n",
      "Iteration no: 149 and Loss: 157.0916748046875\n",
      "Iteration no: 150 and Loss: 156.98284912109375\n",
      "Iteration no: 151 and Loss: 156.8741455078125\n",
      "Iteration no: 152 and Loss: 156.76548767089844\n",
      "Iteration no: 153 and Loss: 156.65692138671875\n",
      "Iteration no: 154 and Loss: 156.54844665527344\n",
      "Iteration no: 155 and Loss: 156.4400177001953\n",
      "Iteration no: 156 and Loss: 156.3317108154297\n",
      "Iteration no: 157 and Loss: 156.2234649658203\n",
      "Iteration no: 158 and Loss: 156.11534118652344\n",
      "Iteration no: 159 and Loss: 156.00718688964844\n",
      "Iteration no: 160 and Loss: 155.8992462158203\n",
      "Iteration no: 161 and Loss: 155.79127502441406\n",
      "Iteration no: 162 and Loss: 155.6834716796875\n",
      "Iteration no: 163 and Loss: 155.57571411132812\n",
      "Iteration no: 164 and Loss: 155.46800231933594\n",
      "Iteration no: 165 and Loss: 155.3603973388672\n",
      "Iteration no: 166 and Loss: 155.2528076171875\n",
      "Iteration no: 167 and Loss: 155.14529418945312\n",
      "Iteration no: 168 and Loss: 155.03775024414062\n",
      "Iteration no: 169 and Loss: 154.9303436279297\n",
      "Iteration no: 170 and Loss: 154.82301330566406\n",
      "Iteration no: 171 and Loss: 154.71572875976562\n",
      "Iteration no: 172 and Loss: 154.6085205078125\n",
      "Iteration no: 173 and Loss: 154.50137329101562\n",
      "Iteration no: 174 and Loss: 154.39431762695312\n",
      "Iteration no: 175 and Loss: 154.28732299804688\n",
      "Iteration no: 176 and Loss: 154.18048095703125\n",
      "Iteration no: 177 and Loss: 154.07357788085938\n",
      "Iteration no: 178 and Loss: 153.9667510986328\n",
      "Iteration no: 179 and Loss: 153.8600616455078\n",
      "Iteration no: 180 and Loss: 153.75343322753906\n",
      "Iteration no: 181 and Loss: 153.6468505859375\n",
      "Iteration no: 182 and Loss: 153.54029846191406\n",
      "Iteration no: 183 and Loss: 153.4339141845703\n",
      "Iteration no: 184 and Loss: 153.32754516601562\n",
      "Iteration no: 185 and Loss: 153.22129821777344\n",
      "Iteration no: 186 and Loss: 153.11509704589844\n",
      "Iteration no: 187 and Loss: 153.00897216796875\n",
      "Iteration no: 188 and Loss: 152.90292358398438\n",
      "Iteration no: 189 and Loss: 152.7969512939453\n",
      "Iteration no: 190 and Loss: 152.6910400390625\n",
      "Iteration no: 191 and Loss: 152.58523559570312\n",
      "Iteration no: 192 and Loss: 152.4794921875\n",
      "Iteration no: 193 and Loss: 152.37380981445312\n",
      "Iteration no: 194 and Loss: 152.2682342529297\n",
      "Iteration no: 195 and Loss: 152.1627197265625\n",
      "Iteration no: 196 and Loss: 152.05728149414062\n",
      "Iteration no: 197 and Loss: 151.9518585205078\n",
      "Iteration no: 198 and Loss: 151.84658813476562\n",
      "Iteration no: 199 and Loss: 151.7413787841797\n",
      "Iteration no: 200 and Loss: 151.63619995117188\n",
      "Iteration no: 201 and Loss: 151.53115844726562\n",
      "Iteration no: 202 and Loss: 151.42611694335938\n",
      "Iteration no: 203 and Loss: 151.3212127685547\n",
      "Iteration no: 204 and Loss: 151.21633911132812\n",
      "Iteration no: 205 and Loss: 151.11160278320312\n",
      "Iteration no: 206 and Loss: 151.00686645507812\n",
      "Iteration no: 207 and Loss: 150.9022979736328\n",
      "Iteration no: 208 and Loss: 150.79769897460938\n",
      "Iteration no: 209 and Loss: 150.69325256347656\n",
      "Iteration no: 210 and Loss: 150.58883666992188\n",
      "Iteration no: 211 and Loss: 150.48451232910156\n",
      "Iteration no: 212 and Loss: 150.38021850585938\n",
      "Iteration no: 213 and Loss: 150.27606201171875\n",
      "Iteration no: 214 and Loss: 150.1719512939453\n",
      "Iteration no: 215 and Loss: 150.0679168701172\n",
      "Iteration no: 216 and Loss: 149.9639129638672\n",
      "Iteration no: 217 and Loss: 149.8600616455078\n",
      "Iteration no: 218 and Loss: 149.75619506835938\n",
      "Iteration no: 219 and Loss: 149.65243530273438\n",
      "Iteration no: 220 and Loss: 149.54869079589844\n",
      "Iteration no: 221 and Loss: 149.4451904296875\n",
      "Iteration no: 222 and Loss: 149.34156799316406\n",
      "Iteration no: 223 and Loss: 149.23818969726562\n",
      "Iteration no: 224 and Loss: 149.13470458984375\n",
      "Iteration no: 225 and Loss: 149.0313720703125\n",
      "Iteration no: 226 and Loss: 148.9281463623047\n",
      "Iteration no: 227 and Loss: 148.824951171875\n",
      "Iteration no: 228 and Loss: 148.72181701660156\n",
      "Iteration no: 229 and Loss: 148.61880493164062\n",
      "Iteration no: 230 and Loss: 148.5157928466797\n",
      "Iteration no: 231 and Loss: 148.4129180908203\n",
      "Iteration no: 232 and Loss: 148.31011962890625\n",
      "Iteration no: 233 and Loss: 148.2073974609375\n",
      "Iteration no: 234 and Loss: 148.10467529296875\n",
      "Iteration no: 235 and Loss: 148.00210571289062\n",
      "Iteration no: 236 and Loss: 147.8995361328125\n",
      "Iteration no: 237 and Loss: 147.7970733642578\n",
      "Iteration no: 238 and Loss: 147.69464111328125\n",
      "Iteration no: 239 and Loss: 147.5923309326172\n",
      "Iteration no: 240 and Loss: 147.49005126953125\n",
      "Iteration no: 241 and Loss: 147.3878936767578\n",
      "Iteration no: 242 and Loss: 147.28579711914062\n",
      "Iteration no: 243 and Loss: 147.18380737304688\n",
      "Iteration no: 244 and Loss: 147.0818634033203\n",
      "Iteration no: 245 and Loss: 146.98004150390625\n",
      "Iteration no: 246 and Loss: 146.8783416748047\n",
      "Iteration no: 247 and Loss: 146.77658081054688\n",
      "Iteration no: 248 and Loss: 146.67501831054688\n",
      "Iteration no: 249 and Loss: 146.57345581054688\n",
      "Iteration no: 250 and Loss: 146.47201538085938\n",
      "Iteration no: 251 and Loss: 146.37063598632812\n",
      "Iteration no: 252 and Loss: 146.26931762695312\n",
      "Iteration no: 253 and Loss: 146.16806030273438\n",
      "Iteration no: 254 and Loss: 146.06689453125\n",
      "Iteration no: 255 and Loss: 145.96578979492188\n",
      "Iteration no: 256 and Loss: 145.86471557617188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration no: 257 and Loss: 145.7637481689453\n",
      "Iteration no: 258 and Loss: 145.66290283203125\n",
      "Iteration no: 259 and Loss: 145.56210327148438\n",
      "Iteration no: 260 and Loss: 145.4613800048828\n",
      "Iteration no: 261 and Loss: 145.36068725585938\n",
      "Iteration no: 262 and Loss: 145.2600555419922\n",
      "Iteration no: 263 and Loss: 145.15956115722656\n",
      "Iteration no: 264 and Loss: 145.05909729003906\n",
      "Iteration no: 265 and Loss: 144.95870971679688\n",
      "Iteration no: 266 and Loss: 144.85841369628906\n",
      "Iteration no: 267 and Loss: 144.7581329345703\n",
      "Iteration no: 268 and Loss: 144.65792846679688\n",
      "Iteration no: 269 and Loss: 144.557861328125\n",
      "Iteration no: 270 and Loss: 144.45785522460938\n",
      "Iteration no: 271 and Loss: 144.35791015625\n",
      "Iteration no: 272 and Loss: 144.2580108642578\n",
      "Iteration no: 273 and Loss: 144.15818786621094\n",
      "Iteration no: 274 and Loss: 144.05844116210938\n",
      "Iteration no: 275 and Loss: 143.95880126953125\n",
      "Iteration no: 276 and Loss: 143.85914611816406\n",
      "Iteration no: 277 and Loss: 143.75961303710938\n",
      "Iteration no: 278 and Loss: 143.66009521484375\n",
      "Iteration no: 279 and Loss: 143.56076049804688\n",
      "Iteration no: 280 and Loss: 143.46142578125\n",
      "Iteration no: 281 and Loss: 143.36209106445312\n",
      "Iteration no: 282 and Loss: 143.26290893554688\n",
      "Iteration no: 283 and Loss: 143.16375732421875\n",
      "Iteration no: 284 and Loss: 143.06466674804688\n",
      "Iteration no: 285 and Loss: 142.96571350097656\n",
      "Iteration no: 286 and Loss: 142.86671447753906\n",
      "Iteration no: 287 and Loss: 142.76783752441406\n",
      "Iteration no: 288 and Loss: 142.66909790039062\n",
      "Iteration no: 289 and Loss: 142.57029724121094\n",
      "Iteration no: 290 and Loss: 142.4716796875\n",
      "Iteration no: 291 and Loss: 142.37307739257812\n",
      "Iteration no: 292 and Loss: 142.27456665039062\n",
      "Iteration no: 293 and Loss: 142.17616271972656\n",
      "Iteration no: 294 and Loss: 142.07781982421875\n",
      "Iteration no: 295 and Loss: 141.97958374023438\n",
      "Iteration no: 296 and Loss: 141.88148498535156\n",
      "Iteration no: 297 and Loss: 141.78338623046875\n",
      "Iteration no: 298 and Loss: 141.68533325195312\n",
      "Iteration no: 299 and Loss: 141.5874481201172\n",
      "Iteration no: 300 and Loss: 141.4895782470703\n",
      "Iteration no: 301 and Loss: 141.39175415039062\n",
      "Iteration no: 302 and Loss: 141.2940673828125\n",
      "Iteration no: 303 and Loss: 141.19647216796875\n",
      "Iteration no: 304 and Loss: 141.098876953125\n",
      "Iteration no: 305 and Loss: 141.00135803222656\n",
      "Iteration no: 306 and Loss: 140.90391540527344\n",
      "Iteration no: 307 and Loss: 140.80654907226562\n",
      "Iteration no: 308 and Loss: 140.70933532714844\n",
      "Iteration no: 309 and Loss: 140.61209106445312\n",
      "Iteration no: 310 and Loss: 140.51495361328125\n",
      "Iteration no: 311 and Loss: 140.4178924560547\n",
      "Iteration no: 312 and Loss: 140.32090759277344\n",
      "Iteration no: 313 and Loss: 140.2239990234375\n",
      "Iteration no: 314 and Loss: 140.12716674804688\n",
      "Iteration no: 315 and Loss: 140.0304412841797\n",
      "Iteration no: 316 and Loss: 139.93380737304688\n",
      "Iteration no: 317 and Loss: 139.837158203125\n",
      "Iteration no: 318 and Loss: 139.74061584472656\n",
      "Iteration no: 319 and Loss: 139.64418029785156\n",
      "Iteration no: 320 and Loss: 139.54779052734375\n",
      "Iteration no: 321 and Loss: 139.45143127441406\n",
      "Iteration no: 322 and Loss: 139.355224609375\n",
      "Iteration no: 323 and Loss: 139.25906372070312\n",
      "Iteration no: 324 and Loss: 139.1630096435547\n",
      "Iteration no: 325 and Loss: 139.0669708251953\n",
      "Iteration no: 326 and Loss: 138.97097778320312\n",
      "Iteration no: 327 and Loss: 138.8750762939453\n",
      "Iteration no: 328 and Loss: 138.77926635742188\n",
      "Iteration no: 329 and Loss: 138.68356323242188\n",
      "Iteration no: 330 and Loss: 138.58790588378906\n",
      "Iteration no: 331 and Loss: 138.4923095703125\n",
      "Iteration no: 332 and Loss: 138.3967742919922\n",
      "Iteration no: 333 and Loss: 138.3013153076172\n",
      "Iteration no: 334 and Loss: 138.20596313476562\n",
      "Iteration no: 335 and Loss: 138.1106414794922\n",
      "Iteration no: 336 and Loss: 138.01541137695312\n",
      "Iteration no: 337 and Loss: 137.9202423095703\n",
      "Iteration no: 338 and Loss: 137.8251190185547\n",
      "Iteration no: 339 and Loss: 137.7301025390625\n",
      "Iteration no: 340 and Loss: 137.63514709472656\n",
      "Iteration no: 341 and Loss: 137.540283203125\n",
      "Iteration no: 342 and Loss: 137.44541931152344\n",
      "Iteration no: 343 and Loss: 137.35060119628906\n",
      "Iteration no: 344 and Loss: 137.2559051513672\n",
      "Iteration no: 345 and Loss: 137.1612091064453\n",
      "Iteration no: 346 and Loss: 137.0666046142578\n",
      "Iteration no: 347 and Loss: 136.9720458984375\n",
      "Iteration no: 348 and Loss: 136.87753295898438\n",
      "Iteration no: 349 and Loss: 136.7831573486328\n",
      "Iteration no: 350 and Loss: 136.68881225585938\n",
      "Iteration no: 351 and Loss: 136.59454345703125\n",
      "Iteration no: 352 and Loss: 136.50033569335938\n",
      "Iteration no: 353 and Loss: 136.40618896484375\n",
      "Iteration no: 354 and Loss: 136.3121337890625\n",
      "Iteration no: 355 and Loss: 136.2181396484375\n",
      "Iteration no: 356 and Loss: 136.12417602539062\n",
      "Iteration no: 357 and Loss: 136.0302734375\n",
      "Iteration no: 358 and Loss: 135.93649291992188\n",
      "Iteration no: 359 and Loss: 135.84275817871094\n",
      "Iteration no: 360 and Loss: 135.74908447265625\n",
      "Iteration no: 361 and Loss: 135.65548706054688\n",
      "Iteration no: 362 and Loss: 135.56195068359375\n",
      "Iteration no: 363 and Loss: 135.46852111816406\n",
      "Iteration no: 364 and Loss: 135.37509155273438\n",
      "Iteration no: 365 and Loss: 135.2817840576172\n",
      "Iteration no: 366 and Loss: 135.1885528564453\n",
      "Iteration no: 367 and Loss: 135.0953826904297\n",
      "Iteration no: 368 and Loss: 135.00222778320312\n",
      "Iteration no: 369 and Loss: 134.90924072265625\n",
      "Iteration no: 370 and Loss: 134.81626892089844\n",
      "Iteration no: 371 and Loss: 134.72340393066406\n",
      "Iteration no: 372 and Loss: 134.63055419921875\n",
      "Iteration no: 373 and Loss: 134.53781127929688\n",
      "Iteration no: 374 and Loss: 134.44512939453125\n",
      "Iteration no: 375 and Loss: 134.35256958007812\n",
      "Iteration no: 376 and Loss: 134.26010131835938\n",
      "Iteration no: 377 and Loss: 134.16763305664062\n",
      "Iteration no: 378 and Loss: 134.07530212402344\n",
      "Iteration no: 379 and Loss: 133.98300170898438\n",
      "Iteration no: 380 and Loss: 133.89080810546875\n",
      "Iteration no: 381 and Loss: 133.7986602783203\n",
      "Iteration no: 382 and Loss: 133.7066192626953\n",
      "Iteration no: 383 and Loss: 133.6146240234375\n",
      "Iteration no: 384 and Loss: 133.522705078125\n",
      "Iteration no: 385 and Loss: 133.43081665039062\n",
      "Iteration no: 386 and Loss: 133.33905029296875\n",
      "Iteration no: 387 and Loss: 133.24729919433594\n",
      "Iteration no: 388 and Loss: 133.15565490722656\n",
      "Iteration no: 389 and Loss: 133.0640869140625\n",
      "Iteration no: 390 and Loss: 132.9725341796875\n",
      "Iteration no: 391 and Loss: 132.88108825683594\n",
      "Iteration no: 392 and Loss: 132.78970336914062\n",
      "Iteration no: 393 and Loss: 132.6984405517578\n",
      "Iteration no: 394 and Loss: 132.60720825195312\n",
      "Iteration no: 395 and Loss: 132.51608276367188\n",
      "Iteration no: 396 and Loss: 132.42498779296875\n",
      "Iteration no: 397 and Loss: 132.33396911621094\n",
      "Iteration no: 398 and Loss: 132.24302673339844\n",
      "Iteration no: 399 and Loss: 132.15216064453125\n",
      "Iteration no: 400 and Loss: 132.06138610839844\n",
      "Iteration no: 401 and Loss: 131.97067260742188\n",
      "Iteration no: 402 and Loss: 131.87997436523438\n",
      "Iteration no: 403 and Loss: 131.7894287109375\n",
      "Iteration no: 404 and Loss: 131.69891357421875\n",
      "Iteration no: 405 and Loss: 131.60850524902344\n",
      "Iteration no: 406 and Loss: 131.5181121826172\n",
      "Iteration no: 407 and Loss: 131.42776489257812\n",
      "Iteration no: 408 and Loss: 131.33746337890625\n",
      "Iteration no: 409 and Loss: 131.24725341796875\n",
      "Iteration no: 410 and Loss: 131.15711975097656\n",
      "Iteration no: 411 and Loss: 131.0670166015625\n",
      "Iteration no: 412 and Loss: 130.97703552246094\n",
      "Iteration no: 413 and Loss: 130.88702392578125\n",
      "Iteration no: 414 and Loss: 130.79713439941406\n",
      "Iteration no: 415 and Loss: 130.70730590820312\n",
      "Iteration no: 416 and Loss: 130.61756896972656\n",
      "Iteration no: 417 and Loss: 130.52792358398438\n",
      "Iteration no: 418 and Loss: 130.43832397460938\n",
      "Iteration no: 419 and Loss: 130.3488006591797\n",
      "Iteration no: 420 and Loss: 130.2593231201172\n",
      "Iteration no: 421 and Loss: 130.16990661621094\n",
      "Iteration no: 422 and Loss: 130.0806427001953\n",
      "Iteration no: 423 and Loss: 129.99139404296875\n",
      "Iteration no: 424 and Loss: 129.90220642089844\n",
      "Iteration no: 425 and Loss: 129.8131103515625\n",
      "Iteration no: 426 and Loss: 129.72409057617188\n",
      "Iteration no: 427 and Loss: 129.6351318359375\n",
      "Iteration no: 428 and Loss: 129.54624938964844\n",
      "Iteration no: 429 and Loss: 129.45742797851562\n",
      "Iteration no: 430 and Loss: 129.36868286132812\n",
      "Iteration no: 431 and Loss: 129.28001403808594\n",
      "Iteration no: 432 and Loss: 129.19142150878906\n",
      "Iteration no: 433 and Loss: 129.1028289794922\n",
      "Iteration no: 434 and Loss: 129.01434326171875\n",
      "Iteration no: 435 and Loss: 128.9259490966797\n",
      "Iteration no: 436 and Loss: 128.8376007080078\n",
      "Iteration no: 437 and Loss: 128.74929809570312\n",
      "Iteration no: 438 and Loss: 128.66104125976562\n",
      "Iteration no: 439 and Loss: 128.5728759765625\n",
      "Iteration no: 440 and Loss: 128.48484802246094\n",
      "Iteration no: 441 and Loss: 128.3968048095703\n",
      "Iteration no: 442 and Loss: 128.3088836669922\n",
      "Iteration no: 443 and Loss: 128.22103881835938\n",
      "Iteration no: 444 and Loss: 128.13323974609375\n",
      "Iteration no: 445 and Loss: 128.0454864501953\n",
      "Iteration no: 446 and Loss: 127.95783233642578\n",
      "Iteration no: 447 and Loss: 127.87027740478516\n",
      "Iteration no: 448 and Loss: 127.78275299072266\n",
      "Iteration no: 449 and Loss: 127.69534301757812\n",
      "Iteration no: 450 and Loss: 127.60797119140625\n",
      "Iteration no: 451 and Loss: 127.52067565917969\n",
      "Iteration no: 452 and Loss: 127.43346405029297\n",
      "Iteration no: 453 and Loss: 127.34634399414062\n",
      "Iteration no: 454 and Loss: 127.25926208496094\n",
      "Iteration no: 455 and Loss: 127.17227172851562\n",
      "Iteration no: 456 and Loss: 127.08536529541016\n",
      "Iteration no: 457 and Loss: 126.99848937988281\n",
      "Iteration no: 458 and Loss: 126.91171264648438\n",
      "Iteration no: 459 and Loss: 126.82504272460938\n",
      "Iteration no: 460 and Loss: 126.73837280273438\n",
      "Iteration no: 461 and Loss: 126.65184020996094\n",
      "Iteration no: 462 and Loss: 126.56534576416016\n",
      "Iteration no: 463 and Loss: 126.47891998291016\n",
      "Iteration no: 464 and Loss: 126.39256286621094\n",
      "Iteration no: 465 and Loss: 126.30628967285156\n",
      "Iteration no: 466 and Loss: 126.2200698852539\n",
      "Iteration no: 467 and Loss: 126.13391876220703\n",
      "Iteration no: 468 and Loss: 126.04788208007812\n",
      "Iteration no: 469 and Loss: 125.96186065673828\n",
      "Iteration no: 470 and Loss: 125.8759765625\n",
      "Iteration no: 471 and Loss: 125.79019165039062\n",
      "Iteration no: 472 and Loss: 125.70447540283203\n",
      "Iteration no: 473 and Loss: 125.61882781982422\n",
      "Iteration no: 474 and Loss: 125.53323364257812\n",
      "Iteration no: 475 and Loss: 125.44774627685547\n",
      "Iteration no: 476 and Loss: 125.36231994628906\n",
      "Iteration no: 477 and Loss: 125.27696990966797\n",
      "Iteration no: 478 and Loss: 125.19166564941406\n",
      "Iteration no: 479 and Loss: 125.10649108886719\n",
      "Iteration no: 480 and Loss: 125.02135467529297\n",
      "Iteration no: 481 and Loss: 124.93630981445312\n",
      "Iteration no: 482 and Loss: 124.85132598876953\n",
      "Iteration no: 483 and Loss: 124.76643371582031\n",
      "Iteration no: 484 and Loss: 124.68161010742188\n",
      "Iteration no: 485 and Loss: 124.59689331054688\n",
      "Iteration no: 486 and Loss: 124.51224517822266\n",
      "Iteration no: 487 and Loss: 124.427734375\n",
      "Iteration no: 488 and Loss: 124.3432846069336\n",
      "Iteration no: 489 and Loss: 124.25891876220703\n",
      "Iteration no: 490 and Loss: 124.17462158203125\n",
      "Iteration no: 491 and Loss: 124.09038543701172\n",
      "Iteration no: 492 and Loss: 124.00627136230469\n",
      "Iteration no: 493 and Loss: 123.92216491699219\n",
      "Iteration no: 494 and Loss: 123.83816528320312\n",
      "Iteration no: 495 and Loss: 123.75423431396484\n",
      "Iteration no: 496 and Loss: 123.67041015625\n",
      "Iteration no: 497 and Loss: 123.58663940429688\n",
      "Iteration no: 498 and Loss: 123.5029296875\n",
      "Iteration no: 499 and Loss: 123.41932678222656\n",
      "Iteration no: 500 and Loss: 123.3358383178711\n",
      "Iteration no: 501 and Loss: 123.25238037109375\n",
      "Iteration no: 502 and Loss: 123.16905975341797\n",
      "Iteration no: 503 and Loss: 123.08580780029297\n",
      "Iteration no: 504 and Loss: 123.00262451171875\n",
      "Iteration no: 505 and Loss: 122.91951751708984\n",
      "Iteration no: 506 and Loss: 122.83649444580078\n",
      "Iteration no: 507 and Loss: 122.75358581542969\n",
      "Iteration no: 508 and Loss: 122.67069244384766\n",
      "Iteration no: 509 and Loss: 122.58790588378906\n",
      "Iteration no: 510 and Loss: 122.50519561767578\n",
      "Iteration no: 511 and Loss: 122.42253112792969\n",
      "Iteration no: 512 and Loss: 122.33995819091797\n",
      "Iteration no: 513 and Loss: 122.25745391845703\n",
      "Iteration no: 514 and Loss: 122.17498779296875\n",
      "Iteration no: 515 and Loss: 122.09262084960938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration no: 516 and Loss: 122.01033020019531\n",
      "Iteration no: 517 and Loss: 121.92810821533203\n",
      "Iteration no: 518 and Loss: 121.84593200683594\n",
      "Iteration no: 519 and Loss: 121.76388549804688\n",
      "Iteration no: 520 and Loss: 121.68185424804688\n",
      "Iteration no: 521 and Loss: 121.5999526977539\n",
      "Iteration no: 522 and Loss: 121.51806640625\n",
      "Iteration no: 523 and Loss: 121.43629455566406\n",
      "Iteration no: 524 and Loss: 121.35457611083984\n",
      "Iteration no: 525 and Loss: 121.2729263305664\n",
      "Iteration no: 526 and Loss: 121.19137573242188\n",
      "Iteration no: 527 and Loss: 121.10986328125\n",
      "Iteration no: 528 and Loss: 121.02843475341797\n",
      "Iteration no: 529 and Loss: 120.94709777832031\n",
      "Iteration no: 530 and Loss: 120.8658447265625\n",
      "Iteration no: 531 and Loss: 120.78462982177734\n",
      "Iteration no: 532 and Loss: 120.7035140991211\n",
      "Iteration no: 533 and Loss: 120.6224365234375\n",
      "Iteration no: 534 and Loss: 120.54145812988281\n",
      "Iteration no: 535 and Loss: 120.4605484008789\n",
      "Iteration no: 536 and Loss: 120.37970733642578\n",
      "Iteration no: 537 and Loss: 120.29893493652344\n",
      "Iteration no: 538 and Loss: 120.21824645996094\n",
      "Iteration no: 539 and Loss: 120.13764953613281\n",
      "Iteration no: 540 and Loss: 120.05708312988281\n",
      "Iteration no: 541 and Loss: 119.97662353515625\n",
      "Iteration no: 542 and Loss: 119.89620971679688\n",
      "Iteration no: 543 and Loss: 119.81591796875\n",
      "Iteration no: 544 and Loss: 119.73565673828125\n",
      "Iteration no: 545 and Loss: 119.65545654296875\n",
      "Iteration no: 546 and Loss: 119.57536315917969\n",
      "Iteration no: 547 and Loss: 119.49534606933594\n",
      "Iteration no: 548 and Loss: 119.41539764404297\n",
      "Iteration no: 549 and Loss: 119.3355484008789\n",
      "Iteration no: 550 and Loss: 119.2557144165039\n",
      "Iteration no: 551 and Loss: 119.17599487304688\n",
      "Iteration no: 552 and Loss: 119.09635162353516\n",
      "Iteration no: 553 and Loss: 119.0167465209961\n",
      "Iteration no: 554 and Loss: 118.9372329711914\n",
      "Iteration no: 555 and Loss: 118.85781860351562\n",
      "Iteration no: 556 and Loss: 118.7784423828125\n",
      "Iteration no: 557 and Loss: 118.69912719726562\n",
      "Iteration no: 558 and Loss: 118.61992645263672\n",
      "Iteration no: 559 and Loss: 118.540771484375\n",
      "Iteration no: 560 and Loss: 118.46171569824219\n",
      "Iteration no: 561 and Loss: 118.38270568847656\n",
      "Iteration no: 562 and Loss: 118.30374908447266\n",
      "Iteration no: 563 and Loss: 118.22492980957031\n",
      "Iteration no: 564 and Loss: 118.14613342285156\n",
      "Iteration no: 565 and Loss: 118.06744384765625\n",
      "Iteration no: 566 and Loss: 117.98880767822266\n",
      "Iteration no: 567 and Loss: 117.91022491455078\n",
      "Iteration no: 568 and Loss: 117.83177185058594\n",
      "Iteration no: 569 and Loss: 117.75337982177734\n",
      "Iteration no: 570 and Loss: 117.675048828125\n",
      "Iteration no: 571 and Loss: 117.59675598144531\n",
      "Iteration no: 572 and Loss: 117.51860809326172\n",
      "Iteration no: 573 and Loss: 117.44049072265625\n",
      "Iteration no: 574 and Loss: 117.3624267578125\n",
      "Iteration no: 575 and Loss: 117.28448486328125\n",
      "Iteration no: 576 and Loss: 117.2065658569336\n",
      "Iteration no: 577 and Loss: 117.12874603271484\n",
      "Iteration no: 578 and Loss: 117.05101013183594\n",
      "Iteration no: 579 and Loss: 116.97333526611328\n",
      "Iteration no: 580 and Loss: 116.89571380615234\n",
      "Iteration no: 581 and Loss: 116.8182144165039\n",
      "Iteration no: 582 and Loss: 116.7407455444336\n",
      "Iteration no: 583 and Loss: 116.66339111328125\n",
      "Iteration no: 584 and Loss: 116.58606719970703\n",
      "Iteration no: 585 and Loss: 116.50884246826172\n",
      "Iteration no: 586 and Loss: 116.4316635131836\n",
      "Iteration no: 587 and Loss: 116.35458374023438\n",
      "Iteration no: 588 and Loss: 116.2775650024414\n",
      "Iteration no: 589 and Loss: 116.20059204101562\n",
      "Iteration no: 590 and Loss: 116.12372589111328\n",
      "Iteration no: 591 and Loss: 116.04695129394531\n",
      "Iteration no: 592 and Loss: 115.97022247314453\n",
      "Iteration no: 593 and Loss: 115.89352416992188\n",
      "Iteration no: 594 and Loss: 115.81694793701172\n",
      "Iteration no: 595 and Loss: 115.74041748046875\n",
      "Iteration no: 596 and Loss: 115.66394805908203\n",
      "Iteration no: 597 and Loss: 115.58760070800781\n",
      "Iteration no: 598 and Loss: 115.51129150390625\n",
      "Iteration no: 599 and Loss: 115.43505859375\n",
      "Iteration no: 600 and Loss: 115.35887145996094\n",
      "Iteration no: 601 and Loss: 115.28276062011719\n",
      "Iteration no: 602 and Loss: 115.20675659179688\n",
      "Iteration no: 603 and Loss: 115.13081359863281\n",
      "Iteration no: 604 and Loss: 115.05490112304688\n",
      "Iteration no: 605 and Loss: 114.97911071777344\n",
      "Iteration no: 606 and Loss: 114.90337371826172\n",
      "Iteration no: 607 and Loss: 114.82772827148438\n",
      "Iteration no: 608 and Loss: 114.75212097167969\n",
      "Iteration no: 609 and Loss: 114.6766357421875\n",
      "Iteration no: 610 and Loss: 114.60118865966797\n",
      "Iteration no: 611 and Loss: 114.52580261230469\n",
      "Iteration no: 612 and Loss: 114.45050811767578\n",
      "Iteration no: 613 and Loss: 114.37530517578125\n",
      "Iteration no: 614 and Loss: 114.30012512207031\n",
      "Iteration no: 615 and Loss: 114.2250747680664\n",
      "Iteration no: 616 and Loss: 114.15003967285156\n",
      "Iteration no: 617 and Loss: 114.07513427734375\n",
      "Iteration no: 618 and Loss: 114.00025177001953\n",
      "Iteration no: 619 and Loss: 113.92546081542969\n",
      "Iteration no: 620 and Loss: 113.8507080078125\n",
      "Iteration no: 621 and Loss: 113.77608489990234\n",
      "Iteration no: 622 and Loss: 113.70149993896484\n",
      "Iteration no: 623 and Loss: 113.62700653076172\n",
      "Iteration no: 624 and Loss: 113.5525894165039\n",
      "Iteration no: 625 and Loss: 113.47821807861328\n",
      "Iteration no: 626 and Loss: 113.40394592285156\n",
      "Iteration no: 627 and Loss: 113.3297348022461\n",
      "Iteration no: 628 and Loss: 113.2555923461914\n",
      "Iteration no: 629 and Loss: 113.18152618408203\n",
      "Iteration no: 630 and Loss: 113.10753631591797\n",
      "Iteration no: 631 and Loss: 113.03362274169922\n",
      "Iteration no: 632 and Loss: 112.95977020263672\n",
      "Iteration no: 633 and Loss: 112.88597869873047\n",
      "Iteration no: 634 and Loss: 112.81229400634766\n",
      "Iteration no: 635 and Loss: 112.7386474609375\n",
      "Iteration no: 636 and Loss: 112.66509246826172\n",
      "Iteration no: 637 and Loss: 112.59158325195312\n",
      "Iteration no: 638 and Loss: 112.51822662353516\n",
      "Iteration no: 639 and Loss: 112.44499969482422\n",
      "Iteration no: 640 and Loss: 112.37181091308594\n",
      "Iteration no: 641 and Loss: 112.29874420166016\n",
      "Iteration no: 642 and Loss: 112.22569274902344\n",
      "Iteration no: 643 and Loss: 112.15274047851562\n",
      "Iteration no: 644 and Loss: 112.07984924316406\n",
      "Iteration no: 645 and Loss: 112.00702667236328\n",
      "Iteration no: 646 and Loss: 111.934326171875\n",
      "Iteration no: 647 and Loss: 111.86164093017578\n",
      "Iteration no: 648 and Loss: 111.7890853881836\n",
      "Iteration no: 649 and Loss: 111.71658325195312\n",
      "Iteration no: 650 and Loss: 111.64415740966797\n",
      "Iteration no: 651 and Loss: 111.5718002319336\n",
      "Iteration no: 652 and Loss: 111.49950408935547\n",
      "Iteration no: 653 and Loss: 111.42730712890625\n",
      "Iteration no: 654 and Loss: 111.35518646240234\n",
      "Iteration no: 655 and Loss: 111.28312683105469\n",
      "Iteration no: 656 and Loss: 111.21116638183594\n",
      "Iteration no: 657 and Loss: 111.1392593383789\n",
      "Iteration no: 658 and Loss: 111.0674057006836\n",
      "Iteration no: 659 and Loss: 110.9956283569336\n",
      "Iteration no: 660 and Loss: 110.9239501953125\n",
      "Iteration no: 661 and Loss: 110.85233306884766\n",
      "Iteration no: 662 and Loss: 110.78079223632812\n",
      "Iteration no: 663 and Loss: 110.70929718017578\n",
      "Iteration no: 664 and Loss: 110.63792419433594\n",
      "Iteration no: 665 and Loss: 110.56657409667969\n",
      "Iteration no: 666 and Loss: 110.49534606933594\n",
      "Iteration no: 667 and Loss: 110.42413330078125\n",
      "Iteration no: 668 and Loss: 110.35302734375\n",
      "Iteration no: 669 and Loss: 110.2820053100586\n",
      "Iteration no: 670 and Loss: 110.21099853515625\n",
      "Iteration no: 671 and Loss: 110.14012908935547\n",
      "Iteration no: 672 and Loss: 110.0693130493164\n",
      "Iteration no: 673 and Loss: 109.99855041503906\n",
      "Iteration no: 674 and Loss: 109.92788696289062\n",
      "Iteration no: 675 and Loss: 109.85725402832031\n",
      "Iteration no: 676 and Loss: 109.78669738769531\n",
      "Iteration no: 677 and Loss: 109.71626281738281\n",
      "Iteration no: 678 and Loss: 109.64584350585938\n",
      "Iteration no: 679 and Loss: 109.57554626464844\n",
      "Iteration no: 680 and Loss: 109.5052719116211\n",
      "Iteration no: 681 and Loss: 109.43512725830078\n",
      "Iteration no: 682 and Loss: 109.36499786376953\n",
      "Iteration no: 683 and Loss: 109.29498291015625\n",
      "Iteration no: 684 and Loss: 109.2249984741211\n",
      "Iteration no: 685 and Loss: 109.15509033203125\n",
      "Iteration no: 686 and Loss: 109.08526611328125\n",
      "Iteration no: 687 and Loss: 109.0155029296875\n",
      "Iteration no: 688 and Loss: 108.94577026367188\n",
      "Iteration no: 689 and Loss: 108.8760986328125\n",
      "Iteration no: 690 and Loss: 108.80652618408203\n",
      "Iteration no: 691 and Loss: 108.73702239990234\n",
      "Iteration no: 692 and Loss: 108.66758728027344\n",
      "Iteration no: 693 and Loss: 108.59821319580078\n",
      "Iteration no: 694 and Loss: 108.5289077758789\n",
      "Iteration no: 695 and Loss: 108.45966339111328\n",
      "Iteration no: 696 and Loss: 108.39054107666016\n",
      "Iteration no: 697 and Loss: 108.32144165039062\n",
      "Iteration no: 698 and Loss: 108.25241088867188\n",
      "Iteration no: 699 and Loss: 108.1834716796875\n",
      "Iteration no: 700 and Loss: 108.11460876464844\n",
      "Iteration no: 701 and Loss: 108.04582214355469\n",
      "Iteration no: 702 and Loss: 107.97708892822266\n",
      "Iteration no: 703 and Loss: 107.90847778320312\n",
      "Iteration no: 704 and Loss: 107.83993530273438\n",
      "Iteration no: 705 and Loss: 107.771484375\n",
      "Iteration no: 706 and Loss: 107.70306396484375\n",
      "Iteration no: 707 and Loss: 107.63472747802734\n",
      "Iteration no: 708 and Loss: 107.56646728515625\n",
      "Iteration no: 709 and Loss: 107.49830627441406\n",
      "Iteration no: 710 and Loss: 107.43017578125\n",
      "Iteration no: 711 and Loss: 107.36217498779297\n",
      "Iteration no: 712 and Loss: 107.29418182373047\n",
      "Iteration no: 713 and Loss: 107.22628021240234\n",
      "Iteration no: 714 and Loss: 107.15845489501953\n",
      "Iteration no: 715 and Loss: 107.0906982421875\n",
      "Iteration no: 716 and Loss: 107.02299499511719\n",
      "Iteration no: 717 and Loss: 106.95535278320312\n",
      "Iteration no: 718 and Loss: 106.88780975341797\n",
      "Iteration no: 719 and Loss: 106.82035064697266\n",
      "Iteration no: 720 and Loss: 106.75291442871094\n",
      "Iteration no: 721 and Loss: 106.68559265136719\n",
      "Iteration no: 722 and Loss: 106.61830139160156\n",
      "Iteration no: 723 and Loss: 106.55113983154297\n",
      "Iteration no: 724 and Loss: 106.48402404785156\n",
      "Iteration no: 725 and Loss: 106.41692352294922\n",
      "Iteration no: 726 and Loss: 106.34994506835938\n",
      "Iteration no: 727 and Loss: 106.28303527832031\n",
      "Iteration no: 728 and Loss: 106.2161865234375\n",
      "Iteration no: 729 and Loss: 106.14945220947266\n",
      "Iteration no: 730 and Loss: 106.08273315429688\n",
      "Iteration no: 731 and Loss: 106.01609802246094\n",
      "Iteration no: 732 and Loss: 105.94957733154297\n",
      "Iteration no: 733 and Loss: 105.88307189941406\n",
      "Iteration no: 734 and Loss: 105.81666564941406\n",
      "Iteration no: 735 and Loss: 105.75033569335938\n",
      "Iteration no: 736 and Loss: 105.6840591430664\n",
      "Iteration no: 737 and Loss: 105.61786651611328\n",
      "Iteration no: 738 and Loss: 105.5517349243164\n",
      "Iteration no: 739 and Loss: 105.48571014404297\n",
      "Iteration no: 740 and Loss: 105.41970825195312\n",
      "Iteration no: 741 and Loss: 105.35379791259766\n",
      "Iteration no: 742 and Loss: 105.2879638671875\n",
      "Iteration no: 743 and Loss: 105.22218322753906\n",
      "Iteration no: 744 and Loss: 105.156494140625\n",
      "Iteration no: 745 and Loss: 105.0908432006836\n",
      "Iteration no: 746 and Loss: 105.02531433105469\n",
      "Iteration no: 747 and Loss: 104.95980834960938\n",
      "Iteration no: 748 and Loss: 104.89441680908203\n",
      "Iteration no: 749 and Loss: 104.82905578613281\n",
      "Iteration no: 750 and Loss: 104.7638168334961\n",
      "Iteration no: 751 and Loss: 104.69863891601562\n",
      "Iteration no: 752 and Loss: 104.63348388671875\n",
      "Iteration no: 753 and Loss: 104.56840515136719\n",
      "Iteration no: 754 and Loss: 104.50337219238281\n",
      "Iteration no: 755 and Loss: 104.4383773803711\n",
      "Iteration no: 756 and Loss: 104.37352752685547\n",
      "Iteration no: 757 and Loss: 104.30875396728516\n",
      "Iteration no: 758 and Loss: 104.24403381347656\n",
      "Iteration no: 759 and Loss: 104.17938232421875\n",
      "Iteration no: 760 and Loss: 104.11479949951172\n",
      "Iteration no: 761 and Loss: 104.0502700805664\n",
      "Iteration no: 762 and Loss: 103.98587036132812\n",
      "Iteration no: 763 and Loss: 103.92149353027344\n",
      "Iteration no: 764 and Loss: 103.85720825195312\n",
      "Iteration no: 765 and Loss: 103.7929916381836\n",
      "Iteration no: 766 and Loss: 103.72884368896484\n",
      "Iteration no: 767 and Loss: 103.66474914550781\n",
      "Iteration no: 768 and Loss: 103.60074615478516\n",
      "Iteration no: 769 and Loss: 103.53682708740234\n",
      "Iteration no: 770 and Loss: 103.47296142578125\n",
      "Iteration no: 771 and Loss: 103.40917205810547\n",
      "Iteration no: 772 and Loss: 103.34542846679688\n",
      "Iteration no: 773 and Loss: 103.28180694580078\n",
      "Iteration no: 774 and Loss: 103.21822357177734\n",
      "Iteration no: 775 and Loss: 103.15469360351562\n",
      "Iteration no: 776 and Loss: 103.09130096435547\n",
      "Iteration no: 777 and Loss: 103.02794647216797\n",
      "Iteration no: 778 and Loss: 102.96464538574219\n",
      "Iteration no: 779 and Loss: 102.90142822265625\n",
      "Iteration no: 780 and Loss: 102.83827209472656\n",
      "Iteration no: 781 and Loss: 102.77517700195312\n",
      "Iteration no: 782 and Loss: 102.7121810913086\n",
      "Iteration no: 783 and Loss: 102.64925384521484\n",
      "Iteration no: 784 and Loss: 102.58641052246094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration no: 785 and Loss: 102.52361297607422\n",
      "Iteration no: 786 and Loss: 102.46090698242188\n",
      "Iteration no: 787 and Loss: 102.39823150634766\n",
      "Iteration no: 788 and Loss: 102.33566284179688\n",
      "Iteration no: 789 and Loss: 102.27316284179688\n",
      "Iteration no: 790 and Loss: 102.21074676513672\n",
      "Iteration no: 791 and Loss: 102.14836883544922\n",
      "Iteration no: 792 and Loss: 102.08610534667969\n",
      "Iteration no: 793 and Loss: 102.02387237548828\n",
      "Iteration no: 794 and Loss: 101.96173095703125\n",
      "Iteration no: 795 and Loss: 101.89965057373047\n",
      "Iteration no: 796 and Loss: 101.83763885498047\n",
      "Iteration no: 797 and Loss: 101.7757339477539\n",
      "Iteration no: 798 and Loss: 101.71385192871094\n",
      "Iteration no: 799 and Loss: 101.65210723876953\n",
      "Iteration no: 800 and Loss: 101.5903549194336\n",
      "Iteration no: 801 and Loss: 101.52871704101562\n",
      "Iteration no: 802 and Loss: 101.46713256835938\n",
      "Iteration no: 803 and Loss: 101.40560913085938\n",
      "Iteration no: 804 and Loss: 101.3442153930664\n",
      "Iteration no: 805 and Loss: 101.28285217285156\n",
      "Iteration no: 806 and Loss: 101.22154998779297\n",
      "Iteration no: 807 and Loss: 101.16033935546875\n",
      "Iteration no: 808 and Loss: 101.09918212890625\n",
      "Iteration no: 809 and Loss: 101.03813934326172\n",
      "Iteration no: 810 and Loss: 100.97711181640625\n",
      "Iteration no: 811 and Loss: 100.91618347167969\n",
      "Iteration no: 812 and Loss: 100.85533142089844\n",
      "Iteration no: 813 and Loss: 100.79454040527344\n",
      "Iteration no: 814 and Loss: 100.73383331298828\n",
      "Iteration no: 815 and Loss: 100.67315673828125\n",
      "Iteration no: 816 and Loss: 100.61258697509766\n",
      "Iteration no: 817 and Loss: 100.55207824707031\n",
      "Iteration no: 818 and Loss: 100.4916763305664\n",
      "Iteration no: 819 and Loss: 100.43132019042969\n",
      "Iteration no: 820 and Loss: 100.37107849121094\n",
      "Iteration no: 821 and Loss: 100.31090545654297\n",
      "Iteration no: 822 and Loss: 100.25077819824219\n",
      "Iteration no: 823 and Loss: 100.19074249267578\n",
      "Iteration no: 824 and Loss: 100.13079071044922\n",
      "Iteration no: 825 and Loss: 100.0708999633789\n",
      "Iteration no: 826 and Loss: 100.01110076904297\n",
      "Iteration no: 827 and Loss: 99.95135498046875\n",
      "Iteration no: 828 and Loss: 99.89168548583984\n",
      "Iteration no: 829 and Loss: 99.83209228515625\n",
      "Iteration no: 830 and Loss: 99.7725601196289\n",
      "Iteration no: 831 and Loss: 99.71311950683594\n",
      "Iteration no: 832 and Loss: 99.65373992919922\n",
      "Iteration no: 833 and Loss: 99.59442138671875\n",
      "Iteration no: 834 and Loss: 99.53517150878906\n",
      "Iteration no: 835 and Loss: 99.47598266601562\n",
      "Iteration no: 836 and Loss: 99.41690063476562\n",
      "Iteration no: 837 and Loss: 99.35784912109375\n",
      "Iteration no: 838 and Loss: 99.29891204833984\n",
      "Iteration no: 839 and Loss: 99.24004364013672\n",
      "Iteration no: 840 and Loss: 99.18121337890625\n",
      "Iteration no: 841 and Loss: 99.12246704101562\n",
      "Iteration no: 842 and Loss: 99.06379699707031\n",
      "Iteration no: 843 and Loss: 99.00519561767578\n",
      "Iteration no: 844 and Loss: 98.9466781616211\n",
      "Iteration no: 845 and Loss: 98.8882064819336\n",
      "Iteration no: 846 and Loss: 98.8298110961914\n",
      "Iteration no: 847 and Loss: 98.77149200439453\n",
      "Iteration no: 848 and Loss: 98.7132339477539\n",
      "Iteration no: 849 and Loss: 98.65507507324219\n",
      "Iteration no: 850 and Loss: 98.59697723388672\n",
      "Iteration no: 851 and Loss: 98.53895568847656\n",
      "Iteration no: 852 and Loss: 98.48100280761719\n",
      "Iteration no: 853 and Loss: 98.42311096191406\n",
      "Iteration no: 854 and Loss: 98.36524963378906\n",
      "Iteration no: 855 and Loss: 98.30750274658203\n",
      "Iteration no: 856 and Loss: 98.24983215332031\n",
      "Iteration no: 857 and Loss: 98.19220733642578\n",
      "Iteration no: 858 and Loss: 98.13465881347656\n",
      "Iteration no: 859 and Loss: 98.07720184326172\n",
      "Iteration no: 860 and Loss: 98.01982879638672\n",
      "Iteration no: 861 and Loss: 97.9625244140625\n",
      "Iteration no: 862 and Loss: 97.90530395507812\n",
      "Iteration no: 863 and Loss: 97.84820556640625\n",
      "Iteration no: 864 and Loss: 97.79110717773438\n",
      "Iteration no: 865 and Loss: 97.73412322998047\n",
      "Iteration no: 866 and Loss: 97.67721557617188\n",
      "Iteration no: 867 and Loss: 97.62034606933594\n",
      "Iteration no: 868 and Loss: 97.56356811523438\n",
      "Iteration no: 869 and Loss: 97.50684356689453\n",
      "Iteration no: 870 and Loss: 97.45022583007812\n",
      "Iteration no: 871 and Loss: 97.39366912841797\n",
      "Iteration no: 872 and Loss: 97.337158203125\n",
      "Iteration no: 873 and Loss: 97.28073120117188\n",
      "Iteration no: 874 and Loss: 97.22441101074219\n",
      "Iteration no: 875 and Loss: 97.16812133789062\n",
      "Iteration no: 876 and Loss: 97.11190795898438\n",
      "Iteration no: 877 and Loss: 97.05579376220703\n",
      "Iteration no: 878 and Loss: 96.99970245361328\n",
      "Iteration no: 879 and Loss: 96.94373321533203\n",
      "Iteration no: 880 and Loss: 96.88777160644531\n",
      "Iteration no: 881 and Loss: 96.83191680908203\n",
      "Iteration no: 882 and Loss: 96.77615356445312\n",
      "Iteration no: 883 and Loss: 96.72042846679688\n",
      "Iteration no: 884 and Loss: 96.66475677490234\n",
      "Iteration no: 885 and Loss: 96.60916900634766\n",
      "Iteration no: 886 and Loss: 96.55367279052734\n",
      "Iteration no: 887 and Loss: 96.49821472167969\n",
      "Iteration no: 888 and Loss: 96.44287872314453\n",
      "Iteration no: 889 and Loss: 96.38756561279297\n",
      "Iteration no: 890 and Loss: 96.33235931396484\n",
      "Iteration no: 891 and Loss: 96.27716827392578\n",
      "Iteration no: 892 and Loss: 96.22209167480469\n",
      "Iteration no: 893 and Loss: 96.16705322265625\n",
      "Iteration no: 894 and Loss: 96.11211395263672\n",
      "Iteration no: 895 and Loss: 96.05723571777344\n",
      "Iteration no: 896 and Loss: 96.00243377685547\n",
      "Iteration no: 897 and Loss: 95.94766998291016\n",
      "Iteration no: 898 and Loss: 95.89299011230469\n",
      "Iteration no: 899 and Loss: 95.83838653564453\n",
      "Iteration no: 900 and Loss: 95.78385162353516\n",
      "Iteration no: 901 and Loss: 95.72940063476562\n",
      "Iteration no: 902 and Loss: 95.67497253417969\n",
      "Iteration no: 903 and Loss: 95.62064361572266\n",
      "Iteration no: 904 and Loss: 95.56639862060547\n",
      "Iteration no: 905 and Loss: 95.51219940185547\n",
      "Iteration no: 906 and Loss: 95.45809173583984\n",
      "Iteration no: 907 and Loss: 95.40404510498047\n",
      "Iteration no: 908 and Loss: 95.35004425048828\n",
      "Iteration no: 909 and Loss: 95.296142578125\n",
      "Iteration no: 910 and Loss: 95.24226379394531\n",
      "Iteration no: 911 and Loss: 95.18849182128906\n",
      "Iteration no: 912 and Loss: 95.13481140136719\n",
      "Iteration no: 913 and Loss: 95.0811996459961\n",
      "Iteration no: 914 and Loss: 95.02766418457031\n",
      "Iteration no: 915 and Loss: 94.97420501708984\n",
      "Iteration no: 916 and Loss: 94.92079162597656\n",
      "Iteration no: 917 and Loss: 94.86746978759766\n",
      "Iteration no: 918 and Loss: 94.8141860961914\n",
      "Iteration no: 919 and Loss: 94.76101684570312\n",
      "Iteration no: 920 and Loss: 94.70790100097656\n",
      "Iteration no: 921 and Loss: 94.65481567382812\n",
      "Iteration no: 922 and Loss: 94.6018295288086\n",
      "Iteration no: 923 and Loss: 94.54893493652344\n",
      "Iteration no: 924 and Loss: 94.49608612060547\n",
      "Iteration no: 925 and Loss: 94.44332122802734\n",
      "Iteration no: 926 and Loss: 94.39061737060547\n",
      "Iteration no: 927 and Loss: 94.33797454833984\n",
      "Iteration no: 928 and Loss: 94.2853775024414\n",
      "Iteration no: 929 and Loss: 94.23287963867188\n",
      "Iteration no: 930 and Loss: 94.18045806884766\n",
      "Iteration no: 931 and Loss: 94.1280746459961\n",
      "Iteration no: 932 and Loss: 94.0758056640625\n",
      "Iteration no: 933 and Loss: 94.02354431152344\n",
      "Iteration no: 934 and Loss: 93.97139739990234\n",
      "Iteration no: 935 and Loss: 93.91931915283203\n",
      "Iteration no: 936 and Loss: 93.8673095703125\n",
      "Iteration no: 937 and Loss: 93.8153076171875\n",
      "Iteration no: 938 and Loss: 93.763427734375\n",
      "Iteration no: 939 and Loss: 93.71160888671875\n",
      "Iteration no: 940 and Loss: 93.65985870361328\n",
      "Iteration no: 941 and Loss: 93.6081771850586\n",
      "Iteration no: 942 and Loss: 93.55654907226562\n",
      "Iteration no: 943 and Loss: 93.50505065917969\n",
      "Iteration no: 944 and Loss: 93.45355224609375\n",
      "Iteration no: 945 and Loss: 93.40213775634766\n",
      "Iteration no: 946 and Loss: 93.35079193115234\n",
      "Iteration no: 947 and Loss: 93.29949188232422\n",
      "Iteration no: 948 and Loss: 93.24830627441406\n",
      "Iteration no: 949 and Loss: 93.1971435546875\n",
      "Iteration no: 950 and Loss: 93.14604949951172\n",
      "Iteration no: 951 and Loss: 93.09504699707031\n",
      "Iteration no: 952 and Loss: 93.04410552978516\n",
      "Iteration no: 953 and Loss: 92.99323272705078\n",
      "Iteration no: 954 and Loss: 92.94239807128906\n",
      "Iteration no: 955 and Loss: 92.89167022705078\n",
      "Iteration no: 956 and Loss: 92.84097290039062\n",
      "Iteration no: 957 and Loss: 92.79036712646484\n",
      "Iteration no: 958 and Loss: 92.73982238769531\n",
      "Iteration no: 959 and Loss: 92.68934631347656\n",
      "Iteration no: 960 and Loss: 92.638916015625\n",
      "Iteration no: 961 and Loss: 92.58858489990234\n",
      "Iteration no: 962 and Loss: 92.53829956054688\n",
      "Iteration no: 963 and Loss: 92.48811340332031\n",
      "Iteration no: 964 and Loss: 92.4379653930664\n",
      "Iteration no: 965 and Loss: 92.38788604736328\n",
      "Iteration no: 966 and Loss: 92.33790588378906\n",
      "Iteration no: 967 and Loss: 92.2879409790039\n",
      "Iteration no: 968 and Loss: 92.2380599975586\n",
      "Iteration no: 969 and Loss: 92.18827819824219\n",
      "Iteration no: 970 and Loss: 92.13854217529297\n",
      "Iteration no: 971 and Loss: 92.08885192871094\n",
      "Iteration no: 972 and Loss: 92.03924560546875\n",
      "Iteration no: 973 and Loss: 91.98969268798828\n",
      "Iteration no: 974 and Loss: 91.94025421142578\n",
      "Iteration no: 975 and Loss: 91.89083862304688\n",
      "Iteration no: 976 and Loss: 91.84149932861328\n",
      "Iteration no: 977 and Loss: 91.79220581054688\n",
      "Iteration no: 978 and Loss: 91.7430191040039\n",
      "Iteration no: 979 and Loss: 91.69389343261719\n",
      "Iteration no: 980 and Loss: 91.64482116699219\n",
      "Iteration no: 981 and Loss: 91.59580993652344\n",
      "Iteration no: 982 and Loss: 91.54688262939453\n",
      "Iteration no: 983 and Loss: 91.498046875\n",
      "Iteration no: 984 and Loss: 91.44923400878906\n",
      "Iteration no: 985 and Loss: 91.40045928955078\n",
      "Iteration no: 986 and Loss: 91.35182189941406\n",
      "Iteration no: 987 and Loss: 91.30321502685547\n",
      "Iteration no: 988 and Loss: 91.25468444824219\n",
      "Iteration no: 989 and Loss: 91.20621490478516\n",
      "Iteration no: 990 and Loss: 91.1578140258789\n",
      "Iteration no: 991 and Loss: 91.1094970703125\n",
      "Iteration no: 992 and Loss: 91.06121826171875\n",
      "Iteration no: 993 and Loss: 91.01300811767578\n",
      "Iteration no: 994 and Loss: 90.96488189697266\n",
      "Iteration no: 995 and Loss: 90.9169921875\n",
      "Iteration no: 996 and Loss: 90.869140625\n",
      "Iteration no: 997 and Loss: 90.82135772705078\n",
      "Iteration no: 998 and Loss: 90.77364349365234\n",
      "Iteration no: 999 and Loss: 90.72599792480469\n",
      "Iteration no: 1000 and Loss: 90.67840576171875\n",
      "Iteration no: 1001 and Loss: 90.63088989257812\n",
      "Iteration no: 1002 and Loss: 90.58343505859375\n",
      "Iteration no: 1003 and Loss: 90.53604125976562\n",
      "Iteration no: 1004 and Loss: 90.48872375488281\n",
      "Iteration no: 1005 and Loss: 90.44145202636719\n",
      "Iteration no: 1006 and Loss: 90.39425659179688\n",
      "Iteration no: 1007 and Loss: 90.34713745117188\n",
      "Iteration no: 1008 and Loss: 90.30006408691406\n",
      "Iteration no: 1009 and Loss: 90.25308227539062\n",
      "Iteration no: 1010 and Loss: 90.2061538696289\n",
      "Iteration no: 1011 and Loss: 90.15927124023438\n",
      "Iteration no: 1012 and Loss: 90.11248016357422\n",
      "Iteration no: 1013 and Loss: 90.06571960449219\n",
      "Iteration no: 1014 and Loss: 90.01902770996094\n",
      "Iteration no: 1015 and Loss: 89.97244262695312\n",
      "Iteration no: 1016 and Loss: 89.92591094970703\n",
      "Iteration no: 1017 and Loss: 89.8794174194336\n",
      "Iteration no: 1018 and Loss: 89.8330078125\n",
      "Iteration no: 1019 and Loss: 89.78668975830078\n",
      "Iteration no: 1020 and Loss: 89.74040985107422\n",
      "Iteration no: 1021 and Loss: 89.69415283203125\n",
      "Iteration no: 1022 and Loss: 89.64804077148438\n",
      "Iteration no: 1023 and Loss: 89.60189819335938\n",
      "Iteration no: 1024 and Loss: 89.5558853149414\n",
      "Iteration no: 1025 and Loss: 89.50996398925781\n",
      "Iteration no: 1026 and Loss: 89.46406555175781\n",
      "Iteration no: 1027 and Loss: 89.41822814941406\n",
      "Iteration no: 1028 and Loss: 89.37246704101562\n",
      "Iteration no: 1029 and Loss: 89.3267822265625\n",
      "Iteration no: 1030 and Loss: 89.28113555908203\n",
      "Iteration no: 1031 and Loss: 89.2355728149414\n",
      "Iteration no: 1032 and Loss: 89.19007873535156\n",
      "Iteration no: 1033 and Loss: 89.1446533203125\n",
      "Iteration no: 1034 and Loss: 89.0992660522461\n",
      "Iteration no: 1035 and Loss: 89.053955078125\n",
      "Iteration no: 1036 and Loss: 89.00868225097656\n",
      "Iteration no: 1037 and Loss: 88.96353149414062\n",
      "Iteration no: 1038 and Loss: 88.91841125488281\n",
      "Iteration no: 1039 and Loss: 88.8733901977539\n",
      "Iteration no: 1040 and Loss: 88.82838439941406\n",
      "Iteration no: 1041 and Loss: 88.7834701538086\n",
      "Iteration no: 1042 and Loss: 88.73860931396484\n",
      "Iteration no: 1043 and Loss: 88.69383239746094\n",
      "Iteration no: 1044 and Loss: 88.64910888671875\n",
      "Iteration no: 1045 and Loss: 88.60446166992188\n",
      "Iteration no: 1046 and Loss: 88.55984497070312\n",
      "Iteration no: 1047 and Loss: 88.51534271240234\n",
      "Iteration no: 1048 and Loss: 88.47085571289062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration no: 1049 and Loss: 88.42645263671875\n",
      "Iteration no: 1050 and Loss: 88.38211822509766\n",
      "Iteration no: 1051 and Loss: 88.33784484863281\n",
      "Iteration no: 1052 and Loss: 88.29362487792969\n",
      "Iteration no: 1053 and Loss: 88.24949645996094\n",
      "Iteration no: 1054 and Loss: 88.20540618896484\n",
      "Iteration no: 1055 and Loss: 88.16139221191406\n",
      "Iteration no: 1056 and Loss: 88.11743927001953\n",
      "Iteration no: 1057 and Loss: 88.07354736328125\n",
      "Iteration no: 1058 and Loss: 88.02973175048828\n",
      "Iteration no: 1059 and Loss: 87.98597717285156\n",
      "Iteration no: 1060 and Loss: 87.9422836303711\n",
      "Iteration no: 1061 and Loss: 87.8986587524414\n",
      "Iteration no: 1062 and Loss: 87.85505676269531\n",
      "Iteration no: 1063 and Loss: 87.81156158447266\n",
      "Iteration no: 1064 and Loss: 87.76812744140625\n",
      "Iteration no: 1065 and Loss: 87.72472381591797\n",
      "Iteration no: 1066 and Loss: 87.68142700195312\n",
      "Iteration no: 1067 and Loss: 87.63818359375\n",
      "Iteration no: 1068 and Loss: 87.59500885009766\n",
      "Iteration no: 1069 and Loss: 87.5518569946289\n",
      "Iteration no: 1070 and Loss: 87.50882720947266\n",
      "Iteration no: 1071 and Loss: 87.4658203125\n",
      "Iteration no: 1072 and Loss: 87.42288970947266\n",
      "Iteration no: 1073 and Loss: 87.3800048828125\n",
      "Iteration no: 1074 and Loss: 87.3371810913086\n",
      "Iteration no: 1075 and Loss: 87.29449462890625\n",
      "Iteration no: 1076 and Loss: 87.25181579589844\n",
      "Iteration no: 1077 and Loss: 87.20921325683594\n",
      "Iteration no: 1078 and Loss: 87.16663360595703\n",
      "Iteration no: 1079 and Loss: 87.1241455078125\n",
      "Iteration no: 1080 and Loss: 87.08170318603516\n",
      "Iteration no: 1081 and Loss: 87.03936767578125\n",
      "Iteration no: 1082 and Loss: 86.99705505371094\n",
      "Iteration no: 1083 and Loss: 86.9548110961914\n",
      "Iteration no: 1084 and Loss: 86.91265106201172\n",
      "Iteration no: 1085 and Loss: 86.8705062866211\n",
      "Iteration no: 1086 and Loss: 86.82847595214844\n",
      "Iteration no: 1087 and Loss: 86.78648376464844\n",
      "Iteration no: 1088 and Loss: 86.74456024169922\n",
      "Iteration no: 1089 and Loss: 86.70272064208984\n",
      "Iteration no: 1090 and Loss: 86.66091918945312\n",
      "Iteration no: 1091 and Loss: 86.61917877197266\n",
      "Iteration no: 1092 and Loss: 86.57752227783203\n",
      "Iteration no: 1093 and Loss: 86.53590393066406\n",
      "Iteration no: 1094 and Loss: 86.49436950683594\n",
      "Iteration no: 1095 and Loss: 86.45286560058594\n",
      "Iteration no: 1096 and Loss: 86.41143798828125\n",
      "Iteration no: 1097 and Loss: 86.37005615234375\n",
      "Iteration no: 1098 and Loss: 86.32876586914062\n",
      "Iteration no: 1099 and Loss: 86.28751373291016\n",
      "Iteration no: 1100 and Loss: 86.24634552001953\n",
      "Iteration no: 1101 and Loss: 86.20523834228516\n",
      "Iteration no: 1102 and Loss: 86.1641845703125\n",
      "Iteration no: 1103 and Loss: 86.1231460571289\n",
      "Iteration no: 1104 and Loss: 86.08222198486328\n",
      "Iteration no: 1105 and Loss: 86.0413589477539\n",
      "Iteration no: 1106 and Loss: 86.00054168701172\n",
      "Iteration no: 1107 and Loss: 85.95978546142578\n",
      "Iteration no: 1108 and Loss: 85.9190902709961\n",
      "Iteration no: 1109 and Loss: 85.87848663330078\n",
      "Iteration no: 1110 and Loss: 85.83789825439453\n",
      "Iteration no: 1111 and Loss: 85.79740142822266\n",
      "Iteration no: 1112 and Loss: 85.7569580078125\n",
      "Iteration no: 1113 and Loss: 85.71659088134766\n",
      "Iteration no: 1114 and Loss: 85.6762466430664\n",
      "Iteration no: 1115 and Loss: 85.63600158691406\n",
      "Iteration no: 1116 and Loss: 85.59580993652344\n",
      "Iteration no: 1117 and Loss: 85.55567169189453\n",
      "Iteration no: 1118 and Loss: 85.51557159423828\n",
      "Iteration no: 1119 and Loss: 85.4755859375\n",
      "Iteration no: 1120 and Loss: 85.43560791015625\n",
      "Iteration no: 1121 and Loss: 85.39575958251953\n",
      "Iteration no: 1122 and Loss: 85.35592651367188\n",
      "Iteration no: 1123 and Loss: 85.316162109375\n",
      "Iteration no: 1124 and Loss: 85.27647399902344\n",
      "Iteration no: 1125 and Loss: 85.23683166503906\n",
      "Iteration no: 1126 and Loss: 85.19725799560547\n",
      "Iteration no: 1127 and Loss: 85.15775299072266\n",
      "Iteration no: 1128 and Loss: 85.11830139160156\n",
      "Iteration no: 1129 and Loss: 85.07894134521484\n",
      "Iteration no: 1130 and Loss: 85.03958892822266\n",
      "Iteration no: 1131 and Loss: 85.00032043457031\n",
      "Iteration no: 1132 and Loss: 84.96110534667969\n",
      "Iteration no: 1133 and Loss: 84.9219970703125\n",
      "Iteration no: 1134 and Loss: 84.88289642333984\n",
      "Iteration no: 1135 and Loss: 84.84386444091797\n",
      "Iteration no: 1136 and Loss: 84.8049087524414\n",
      "Iteration no: 1137 and Loss: 84.76603698730469\n",
      "Iteration no: 1138 and Loss: 84.72716522216797\n",
      "Iteration no: 1139 and Loss: 84.68840789794922\n",
      "Iteration no: 1140 and Loss: 84.64968872070312\n",
      "Iteration no: 1141 and Loss: 84.61100006103516\n",
      "Iteration no: 1142 and Loss: 84.57241821289062\n",
      "Iteration no: 1143 and Loss: 84.53385162353516\n",
      "Iteration no: 1144 and Loss: 84.495361328125\n",
      "Iteration no: 1145 and Loss: 84.45694732666016\n",
      "Iteration no: 1146 and Loss: 84.4185791015625\n",
      "Iteration no: 1147 and Loss: 84.38027954101562\n",
      "Iteration no: 1148 and Loss: 84.34200286865234\n",
      "Iteration no: 1149 and Loss: 84.3038101196289\n",
      "Iteration no: 1150 and Loss: 84.26570129394531\n",
      "Iteration no: 1151 and Loss: 84.22761535644531\n",
      "Iteration no: 1152 and Loss: 84.18962097167969\n",
      "Iteration no: 1153 and Loss: 84.15167236328125\n",
      "Iteration no: 1154 and Loss: 84.11377716064453\n",
      "Iteration no: 1155 and Loss: 84.07598114013672\n",
      "Iteration no: 1156 and Loss: 84.03820037841797\n",
      "Iteration no: 1157 and Loss: 84.00050354003906\n",
      "Iteration no: 1158 and Loss: 83.9629135131836\n",
      "Iteration no: 1159 and Loss: 83.92530822753906\n",
      "Iteration no: 1160 and Loss: 83.88778686523438\n",
      "Iteration no: 1161 and Loss: 83.85031127929688\n",
      "Iteration no: 1162 and Loss: 83.81291961669922\n",
      "Iteration no: 1163 and Loss: 83.77557373046875\n",
      "Iteration no: 1164 and Loss: 83.73829650878906\n",
      "Iteration no: 1165 and Loss: 83.70106506347656\n",
      "Iteration no: 1166 and Loss: 83.66395568847656\n",
      "Iteration no: 1167 and Loss: 83.6268310546875\n",
      "Iteration no: 1168 and Loss: 83.58978271484375\n",
      "Iteration no: 1169 and Loss: 83.55280303955078\n",
      "Iteration no: 1170 and Loss: 83.5158920288086\n",
      "Iteration no: 1171 and Loss: 83.47904205322266\n",
      "Iteration no: 1172 and Loss: 83.44222259521484\n",
      "Iteration no: 1173 and Loss: 83.4054946899414\n",
      "Iteration no: 1174 and Loss: 83.36881256103516\n",
      "Iteration no: 1175 and Loss: 83.33218383789062\n",
      "Iteration no: 1176 and Loss: 83.2956314086914\n",
      "Iteration no: 1177 and Loss: 83.2591323852539\n",
      "Iteration no: 1178 and Loss: 83.22266387939453\n",
      "Iteration no: 1179 and Loss: 83.1863021850586\n",
      "Iteration no: 1180 and Loss: 83.14997863769531\n",
      "Iteration no: 1181 and Loss: 83.11369323730469\n",
      "Iteration no: 1182 and Loss: 83.07748413085938\n",
      "Iteration no: 1183 and Loss: 83.04132080078125\n",
      "Iteration no: 1184 and Loss: 83.0052261352539\n",
      "Iteration no: 1185 and Loss: 82.96922302246094\n",
      "Iteration no: 1186 and Loss: 82.93321990966797\n",
      "Iteration no: 1187 and Loss: 82.89727783203125\n",
      "Iteration no: 1188 and Loss: 82.8614273071289\n",
      "Iteration no: 1189 and Loss: 82.82564544677734\n",
      "Iteration no: 1190 and Loss: 82.7898941040039\n",
      "Iteration no: 1191 and Loss: 82.75421905517578\n",
      "Iteration no: 1192 and Loss: 82.71855926513672\n",
      "Iteration no: 1193 and Loss: 82.6830062866211\n",
      "Iteration no: 1194 and Loss: 82.6474838256836\n",
      "Iteration no: 1195 and Loss: 82.61204528808594\n",
      "Iteration no: 1196 and Loss: 82.57669067382812\n",
      "Iteration no: 1197 and Loss: 82.54132843017578\n",
      "Iteration no: 1198 and Loss: 82.50604248046875\n",
      "Iteration no: 1199 and Loss: 82.4708023071289\n",
      "Iteration no: 1200 and Loss: 82.4356689453125\n",
      "Iteration no: 1201 and Loss: 82.40055847167969\n",
      "Iteration no: 1202 and Loss: 82.3655014038086\n",
      "Iteration no: 1203 and Loss: 82.33052062988281\n",
      "Iteration no: 1204 and Loss: 82.29557800292969\n",
      "Iteration no: 1205 and Loss: 82.26071166992188\n",
      "Iteration no: 1206 and Loss: 82.22588348388672\n",
      "Iteration no: 1207 and Loss: 82.1911392211914\n",
      "Iteration no: 1208 and Loss: 82.15641784667969\n",
      "Iteration no: 1209 and Loss: 82.12178802490234\n",
      "Iteration no: 1210 and Loss: 82.08721160888672\n",
      "Iteration no: 1211 and Loss: 82.05265045166016\n",
      "Iteration no: 1212 and Loss: 82.01819610595703\n",
      "Iteration no: 1213 and Loss: 81.98379516601562\n",
      "Iteration no: 1214 and Loss: 81.94941711425781\n",
      "Iteration no: 1215 and Loss: 81.91514587402344\n",
      "Iteration no: 1216 and Loss: 81.88088989257812\n",
      "Iteration no: 1217 and Loss: 81.8466796875\n",
      "Iteration no: 1218 and Loss: 81.81258392333984\n",
      "Iteration no: 1219 and Loss: 81.77848815917969\n",
      "Iteration no: 1220 and Loss: 81.74449157714844\n",
      "Iteration no: 1221 and Loss: 81.71052551269531\n",
      "Iteration no: 1222 and Loss: 81.6766586303711\n",
      "Iteration no: 1223 and Loss: 81.64279174804688\n",
      "Iteration no: 1224 and Loss: 81.60899353027344\n",
      "Iteration no: 1225 and Loss: 81.57527923583984\n",
      "Iteration no: 1226 and Loss: 81.5416030883789\n",
      "Iteration no: 1227 and Loss: 81.50798797607422\n",
      "Iteration no: 1228 and Loss: 81.47444152832031\n",
      "Iteration no: 1229 and Loss: 81.44093322753906\n",
      "Iteration no: 1230 and Loss: 81.40748596191406\n",
      "Iteration no: 1231 and Loss: 81.37409210205078\n",
      "Iteration no: 1232 and Loss: 81.34077453613281\n",
      "Iteration no: 1233 and Loss: 81.30748748779297\n",
      "Iteration no: 1234 and Loss: 81.27427673339844\n",
      "Iteration no: 1235 and Loss: 81.24112701416016\n",
      "Iteration no: 1236 and Loss: 81.20799255371094\n",
      "Iteration no: 1237 and Loss: 81.17494201660156\n",
      "Iteration no: 1238 and Loss: 81.14196014404297\n",
      "Iteration no: 1239 and Loss: 81.10906219482422\n",
      "Iteration no: 1240 and Loss: 81.07613372802734\n",
      "Iteration no: 1241 and Loss: 81.04334259033203\n",
      "Iteration no: 1242 and Loss: 81.01058959960938\n",
      "Iteration no: 1243 and Loss: 80.97792053222656\n",
      "Iteration no: 1244 and Loss: 80.94525146484375\n",
      "Iteration no: 1245 and Loss: 80.91265869140625\n",
      "Iteration no: 1246 and Loss: 80.880126953125\n",
      "Iteration no: 1247 and Loss: 80.84764862060547\n",
      "Iteration no: 1248 and Loss: 80.81520080566406\n",
      "Iteration no: 1249 and Loss: 80.78286743164062\n",
      "Iteration no: 1250 and Loss: 80.75053405761719\n",
      "Iteration no: 1251 and Loss: 80.71829223632812\n",
      "Iteration no: 1252 and Loss: 80.68608093261719\n",
      "Iteration no: 1253 and Loss: 80.65392303466797\n",
      "Iteration no: 1254 and Loss: 80.6218490600586\n",
      "Iteration no: 1255 and Loss: 80.58981323242188\n",
      "Iteration no: 1256 and Loss: 80.55783081054688\n",
      "Iteration no: 1257 and Loss: 80.5259017944336\n",
      "Iteration no: 1258 and Loss: 80.49406433105469\n",
      "Iteration no: 1259 and Loss: 80.46223449707031\n",
      "Iteration no: 1260 and Loss: 80.43048095703125\n",
      "Iteration no: 1261 and Loss: 80.39879608154297\n",
      "Iteration no: 1262 and Loss: 80.36719512939453\n",
      "Iteration no: 1263 and Loss: 80.33562469482422\n",
      "Iteration no: 1264 and Loss: 80.3041000366211\n",
      "Iteration no: 1265 and Loss: 80.27265167236328\n",
      "Iteration no: 1266 and Loss: 80.2412338256836\n",
      "Iteration no: 1267 and Loss: 80.20989990234375\n",
      "Iteration no: 1268 and Loss: 80.17862701416016\n",
      "Iteration no: 1269 and Loss: 80.14735412597656\n",
      "Iteration no: 1270 and Loss: 80.11614990234375\n",
      "Iteration no: 1271 and Loss: 80.08502197265625\n",
      "Iteration no: 1272 and Loss: 80.05393981933594\n",
      "Iteration no: 1273 and Loss: 80.02289581298828\n",
      "Iteration no: 1274 and Loss: 79.99192810058594\n",
      "Iteration no: 1275 and Loss: 79.96098327636719\n",
      "Iteration no: 1276 and Loss: 79.93013000488281\n",
      "Iteration no: 1277 and Loss: 79.89929962158203\n",
      "Iteration no: 1278 and Loss: 79.8685302734375\n",
      "Iteration no: 1279 and Loss: 79.83784484863281\n",
      "Iteration no: 1280 and Loss: 79.80717468261719\n",
      "Iteration no: 1281 and Loss: 79.77657318115234\n",
      "Iteration no: 1282 and Loss: 79.74604034423828\n",
      "Iteration no: 1283 and Loss: 79.71556091308594\n",
      "Iteration no: 1284 and Loss: 79.68509674072266\n",
      "Iteration no: 1285 and Loss: 79.65470886230469\n",
      "Iteration no: 1286 and Loss: 79.62439727783203\n",
      "Iteration no: 1287 and Loss: 79.59410095214844\n",
      "Iteration no: 1288 and Loss: 79.56388092041016\n",
      "Iteration no: 1289 and Loss: 79.53373718261719\n",
      "Iteration no: 1290 and Loss: 79.50363159179688\n",
      "Iteration no: 1291 and Loss: 79.47355651855469\n",
      "Iteration no: 1292 and Loss: 79.44355010986328\n",
      "Iteration no: 1293 and Loss: 79.41357421875\n",
      "Iteration no: 1294 and Loss: 79.38370513916016\n",
      "Iteration no: 1295 and Loss: 79.35385131835938\n",
      "Iteration no: 1296 and Loss: 79.32403564453125\n",
      "Iteration no: 1297 and Loss: 79.2942886352539\n",
      "Iteration no: 1298 and Loss: 79.26459503173828\n",
      "Iteration no: 1299 and Loss: 79.23494720458984\n",
      "Iteration no: 1300 and Loss: 79.20537567138672\n",
      "Iteration no: 1301 and Loss: 79.17585754394531\n",
      "Iteration no: 1302 and Loss: 79.14634704589844\n",
      "Iteration no: 1303 and Loss: 79.11690521240234\n",
      "Iteration no: 1304 and Loss: 79.08753967285156\n",
      "Iteration no: 1305 and Loss: 79.05823516845703\n",
      "Iteration no: 1306 and Loss: 79.0289535522461\n",
      "Iteration no: 1307 and Loss: 78.99974060058594\n",
      "Iteration no: 1308 and Loss: 78.97054290771484\n",
      "Iteration no: 1309 and Loss: 78.94145202636719\n",
      "Iteration no: 1310 and Loss: 78.91238403320312\n",
      "Iteration no: 1311 and Loss: 78.88338470458984\n",
      "Iteration no: 1312 and Loss: 78.85443115234375\n",
      "Iteration no: 1313 and Loss: 78.82553100585938\n",
      "Iteration no: 1314 and Loss: 78.79667663574219\n",
      "Iteration no: 1315 and Loss: 78.76789093017578\n",
      "Iteration no: 1316 and Loss: 78.7391357421875\n",
      "Iteration no: 1317 and Loss: 78.71043395996094\n",
      "Iteration no: 1318 and Loss: 78.6817855834961\n",
      "Iteration no: 1319 and Loss: 78.65321350097656\n",
      "Iteration no: 1320 and Loss: 78.62467956542969\n",
      "Iteration no: 1321 and Loss: 78.59619140625\n",
      "Iteration no: 1322 and Loss: 78.56773376464844\n",
      "Iteration no: 1323 and Loss: 78.53937530517578\n",
      "Iteration no: 1324 and Loss: 78.51104736328125\n",
      "Iteration no: 1325 and Loss: 78.48279571533203\n",
      "Iteration no: 1326 and Loss: 78.45454406738281\n",
      "Iteration no: 1327 and Loss: 78.42637634277344\n",
      "Iteration no: 1328 and Loss: 78.39825439453125\n",
      "Iteration no: 1329 and Loss: 78.37019348144531\n",
      "Iteration no: 1330 and Loss: 78.3421630859375\n",
      "Iteration no: 1331 and Loss: 78.31420135498047\n",
      "Iteration no: 1332 and Loss: 78.28630828857422\n",
      "Iteration no: 1333 and Loss: 78.25848388671875\n",
      "Iteration no: 1334 and Loss: 78.2306900024414\n",
      "Iteration no: 1335 and Loss: 78.20291900634766\n",
      "Iteration no: 1336 and Loss: 78.17523193359375\n",
      "Iteration no: 1337 and Loss: 78.14759063720703\n",
      "Iteration no: 1338 and Loss: 78.12001037597656\n",
      "Iteration no: 1339 and Loss: 78.09249877929688\n",
      "Iteration no: 1340 and Loss: 78.06501770019531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration no: 1341 and Loss: 78.03756713867188\n",
      "Iteration no: 1342 and Loss: 78.01018524169922\n",
      "Iteration no: 1343 and Loss: 77.98287963867188\n",
      "Iteration no: 1344 and Loss: 77.9555892944336\n",
      "Iteration no: 1345 and Loss: 77.92837524414062\n",
      "Iteration no: 1346 and Loss: 77.90120697021484\n",
      "Iteration no: 1347 and Loss: 77.87406921386719\n",
      "Iteration no: 1348 and Loss: 77.8470230102539\n",
      "Iteration no: 1349 and Loss: 77.81999969482422\n",
      "Iteration no: 1350 and Loss: 77.79304504394531\n",
      "Iteration no: 1351 and Loss: 77.76612091064453\n",
      "Iteration no: 1352 and Loss: 77.73926544189453\n",
      "Iteration no: 1353 and Loss: 77.71247100830078\n",
      "Iteration no: 1354 and Loss: 77.68570709228516\n",
      "Iteration no: 1355 and Loss: 77.65904235839844\n",
      "Iteration no: 1356 and Loss: 77.63236999511719\n",
      "Iteration no: 1357 and Loss: 77.60575103759766\n",
      "Iteration no: 1358 and Loss: 77.57923889160156\n",
      "Iteration no: 1359 and Loss: 77.55272674560547\n",
      "Iteration no: 1360 and Loss: 77.5262680053711\n",
      "Iteration no: 1361 and Loss: 77.49987030029297\n",
      "Iteration no: 1362 and Loss: 77.47352600097656\n",
      "Iteration no: 1363 and Loss: 77.44722747802734\n",
      "Iteration no: 1364 and Loss: 77.42098236083984\n",
      "Iteration no: 1365 and Loss: 77.39476013183594\n",
      "Iteration no: 1366 and Loss: 77.36863708496094\n",
      "Iteration no: 1367 and Loss: 77.34251403808594\n",
      "Iteration no: 1368 and Loss: 77.31649780273438\n",
      "Iteration no: 1369 and Loss: 77.29048919677734\n",
      "Iteration no: 1370 and Loss: 77.26451110839844\n",
      "Iteration no: 1371 and Loss: 77.23863220214844\n",
      "Iteration no: 1372 and Loss: 77.21275329589844\n",
      "Iteration no: 1373 and Loss: 77.18698120117188\n",
      "Iteration no: 1374 and Loss: 77.16120147705078\n",
      "Iteration no: 1375 and Loss: 77.13551330566406\n",
      "Iteration no: 1376 and Loss: 77.10986328125\n",
      "Iteration no: 1377 and Loss: 77.0842514038086\n",
      "Iteration no: 1378 and Loss: 77.0586929321289\n",
      "Iteration no: 1379 and Loss: 77.03319549560547\n",
      "Iteration no: 1380 and Loss: 77.00774383544922\n",
      "Iteration no: 1381 and Loss: 76.98234558105469\n",
      "Iteration no: 1382 and Loss: 76.95696258544922\n",
      "Iteration no: 1383 and Loss: 76.93164825439453\n",
      "Iteration no: 1384 and Loss: 76.9063949584961\n",
      "Iteration no: 1385 and Loss: 76.88117980957031\n",
      "Iteration no: 1386 and Loss: 76.85604095458984\n",
      "Iteration no: 1387 and Loss: 76.83091735839844\n",
      "Iteration no: 1388 and Loss: 76.80585479736328\n",
      "Iteration no: 1389 and Loss: 76.78082275390625\n",
      "Iteration no: 1390 and Loss: 76.755859375\n",
      "Iteration no: 1391 and Loss: 76.73094177246094\n",
      "Iteration no: 1392 and Loss: 76.70608520507812\n",
      "Iteration no: 1393 and Loss: 76.68125915527344\n",
      "Iteration no: 1394 and Loss: 76.6564712524414\n",
      "Iteration no: 1395 and Loss: 76.63175201416016\n",
      "Iteration no: 1396 and Loss: 76.60704040527344\n",
      "Iteration no: 1397 and Loss: 76.58242797851562\n",
      "Iteration no: 1398 and Loss: 76.55786895751953\n",
      "Iteration no: 1399 and Loss: 76.53330993652344\n",
      "Iteration no: 1400 and Loss: 76.50881958007812\n",
      "Iteration no: 1401 and Loss: 76.484375\n",
      "Iteration no: 1402 and Loss: 76.46000671386719\n",
      "Iteration no: 1403 and Loss: 76.43567657470703\n",
      "Iteration no: 1404 and Loss: 76.41139221191406\n",
      "Iteration no: 1405 and Loss: 76.38712310791016\n",
      "Iteration no: 1406 and Loss: 76.3629379272461\n",
      "Iteration no: 1407 and Loss: 76.33878326416016\n",
      "Iteration no: 1408 and Loss: 76.31468200683594\n",
      "Iteration no: 1409 and Loss: 76.29060363769531\n",
      "Iteration no: 1410 and Loss: 76.26661682128906\n",
      "Iteration no: 1411 and Loss: 76.2426528930664\n",
      "Iteration no: 1412 and Loss: 76.21874237060547\n",
      "Iteration no: 1413 and Loss: 76.19487762451172\n",
      "Iteration no: 1414 and Loss: 76.17107391357422\n",
      "Iteration no: 1415 and Loss: 76.14730834960938\n",
      "Iteration no: 1416 and Loss: 76.12356567382812\n",
      "Iteration no: 1417 and Loss: 76.09990692138672\n",
      "Iteration no: 1418 and Loss: 76.07625579833984\n",
      "Iteration no: 1419 and Loss: 76.05265808105469\n",
      "Iteration no: 1420 and Loss: 76.02916717529297\n",
      "Iteration no: 1421 and Loss: 76.00566101074219\n",
      "Iteration no: 1422 and Loss: 75.9822006225586\n",
      "Iteration no: 1423 and Loss: 75.95881652832031\n",
      "Iteration no: 1424 and Loss: 75.93546295166016\n",
      "Iteration no: 1425 and Loss: 75.91218566894531\n",
      "Iteration no: 1426 and Loss: 75.88890838623047\n",
      "Iteration no: 1427 and Loss: 75.86572265625\n",
      "Iteration no: 1428 and Loss: 75.842529296875\n",
      "Iteration no: 1429 and Loss: 75.81942749023438\n",
      "Iteration no: 1430 and Loss: 75.79635620117188\n",
      "Iteration no: 1431 and Loss: 75.77333068847656\n",
      "Iteration no: 1432 and Loss: 75.75032043457031\n",
      "Iteration no: 1433 and Loss: 75.72740936279297\n",
      "Iteration no: 1434 and Loss: 75.70452117919922\n",
      "Iteration no: 1435 and Loss: 75.68172454833984\n",
      "Iteration no: 1436 and Loss: 75.65889739990234\n",
      "Iteration no: 1437 and Loss: 75.63616180419922\n",
      "Iteration no: 1438 and Loss: 75.61348724365234\n",
      "Iteration no: 1439 and Loss: 75.59080505371094\n",
      "Iteration no: 1440 and Loss: 75.56820678710938\n",
      "Iteration no: 1441 and Loss: 75.5456314086914\n",
      "Iteration no: 1442 and Loss: 75.52313232421875\n",
      "Iteration no: 1443 and Loss: 75.50064849853516\n",
      "Iteration no: 1444 and Loss: 75.47822570800781\n",
      "Iteration no: 1445 and Loss: 75.45582580566406\n",
      "Iteration no: 1446 and Loss: 75.43350219726562\n",
      "Iteration no: 1447 and Loss: 75.4112319946289\n",
      "Iteration no: 1448 and Loss: 75.38899230957031\n",
      "Iteration no: 1449 and Loss: 75.36676788330078\n",
      "Iteration no: 1450 and Loss: 75.34459686279297\n",
      "Iteration no: 1451 and Loss: 75.32249450683594\n",
      "Iteration no: 1452 and Loss: 75.30042266845703\n",
      "Iteration no: 1453 and Loss: 75.27839660644531\n",
      "Iteration no: 1454 and Loss: 75.25643157958984\n",
      "Iteration no: 1455 and Loss: 75.23448944091797\n",
      "Iteration no: 1456 and Loss: 75.2126235961914\n",
      "Iteration no: 1457 and Loss: 75.19078063964844\n",
      "Iteration no: 1458 and Loss: 75.16897583007812\n",
      "Iteration no: 1459 and Loss: 75.14720153808594\n",
      "Iteration no: 1460 and Loss: 75.1255111694336\n",
      "Iteration no: 1461 and Loss: 75.10384368896484\n",
      "Iteration no: 1462 and Loss: 75.08222961425781\n",
      "Iteration no: 1463 and Loss: 75.06063842773438\n",
      "Iteration no: 1464 and Loss: 75.03909301757812\n",
      "Iteration no: 1465 and Loss: 75.01761627197266\n",
      "Iteration no: 1466 and Loss: 74.99618530273438\n",
      "Iteration no: 1467 and Loss: 74.97480010986328\n",
      "Iteration no: 1468 and Loss: 74.95342254638672\n",
      "Iteration no: 1469 and Loss: 74.93209838867188\n",
      "Iteration no: 1470 and Loss: 74.91085052490234\n",
      "Iteration no: 1471 and Loss: 74.88959503173828\n",
      "Iteration no: 1472 and Loss: 74.86841583251953\n",
      "Iteration no: 1473 and Loss: 74.84727478027344\n",
      "Iteration no: 1474 and Loss: 74.82618713378906\n",
      "Iteration no: 1475 and Loss: 74.80512237548828\n",
      "Iteration no: 1476 and Loss: 74.78412628173828\n",
      "Iteration no: 1477 and Loss: 74.76316833496094\n",
      "Iteration no: 1478 and Loss: 74.74224090576172\n",
      "Iteration no: 1479 and Loss: 74.72136688232422\n",
      "Iteration no: 1480 and Loss: 74.70054626464844\n",
      "Iteration no: 1481 and Loss: 74.67973327636719\n",
      "Iteration no: 1482 and Loss: 74.65900421142578\n",
      "Iteration no: 1483 and Loss: 74.63829040527344\n",
      "Iteration no: 1484 and Loss: 74.61763763427734\n",
      "Iteration no: 1485 and Loss: 74.59700012207031\n",
      "Iteration no: 1486 and Loss: 74.57642364501953\n",
      "Iteration no: 1487 and Loss: 74.55590057373047\n",
      "Iteration no: 1488 and Loss: 74.5353775024414\n",
      "Iteration no: 1489 and Loss: 74.51496124267578\n",
      "Iteration no: 1490 and Loss: 74.49454498291016\n",
      "Iteration no: 1491 and Loss: 74.47415161132812\n",
      "Iteration no: 1492 and Loss: 74.45387268066406\n",
      "Iteration no: 1493 and Loss: 74.43357849121094\n",
      "Iteration no: 1494 and Loss: 74.41334533691406\n",
      "Iteration no: 1495 and Loss: 74.39315032958984\n",
      "Iteration no: 1496 and Loss: 74.3730239868164\n",
      "Iteration no: 1497 and Loss: 74.35292053222656\n",
      "Iteration no: 1498 and Loss: 74.33283996582031\n",
      "Iteration no: 1499 and Loss: 74.3128433227539\n",
      "Iteration no: 1500 and Loss: 74.29293060302734\n",
      "Iteration no: 1501 and Loss: 74.2730941772461\n",
      "Iteration no: 1502 and Loss: 74.25326538085938\n",
      "Iteration no: 1503 and Loss: 74.23350524902344\n",
      "Iteration no: 1504 and Loss: 74.21378326416016\n",
      "Iteration no: 1505 and Loss: 74.19409942626953\n",
      "Iteration no: 1506 and Loss: 74.17446899414062\n",
      "Iteration no: 1507 and Loss: 74.15483856201172\n",
      "Iteration no: 1508 and Loss: 74.13533020019531\n",
      "Iteration no: 1509 and Loss: 74.1158218383789\n",
      "Iteration no: 1510 and Loss: 74.09634399414062\n",
      "Iteration no: 1511 and Loss: 74.07693481445312\n",
      "Iteration no: 1512 and Loss: 74.05755615234375\n",
      "Iteration no: 1513 and Loss: 74.03822326660156\n",
      "Iteration no: 1514 and Loss: 74.01895141601562\n",
      "Iteration no: 1515 and Loss: 73.99967956542969\n",
      "Iteration no: 1516 and Loss: 73.98045349121094\n",
      "Iteration no: 1517 and Loss: 73.96131896972656\n",
      "Iteration no: 1518 and Loss: 73.94219207763672\n",
      "Iteration no: 1519 and Loss: 73.92310333251953\n",
      "Iteration no: 1520 and Loss: 73.90409088134766\n",
      "Iteration no: 1521 and Loss: 73.88510131835938\n",
      "Iteration no: 1522 and Loss: 73.86614990234375\n",
      "Iteration no: 1523 and Loss: 73.84722137451172\n",
      "Iteration no: 1524 and Loss: 73.828369140625\n",
      "Iteration no: 1525 and Loss: 73.8095474243164\n",
      "Iteration no: 1526 and Loss: 73.79074096679688\n",
      "Iteration no: 1527 and Loss: 73.77201843261719\n",
      "Iteration no: 1528 and Loss: 73.75331115722656\n",
      "Iteration no: 1529 and Loss: 73.73466491699219\n",
      "Iteration no: 1530 and Loss: 73.71601104736328\n",
      "Iteration no: 1531 and Loss: 73.6974105834961\n",
      "Iteration no: 1532 and Loss: 73.67889404296875\n",
      "Iteration no: 1533 and Loss: 73.66039276123047\n",
      "Iteration no: 1534 and Loss: 73.64193725585938\n",
      "Iteration no: 1535 and Loss: 73.6235122680664\n",
      "Iteration no: 1536 and Loss: 73.60514831542969\n",
      "Iteration no: 1537 and Loss: 73.58676147460938\n",
      "Iteration no: 1538 and Loss: 73.5684814453125\n",
      "Iteration no: 1539 and Loss: 73.55024719238281\n",
      "Iteration no: 1540 and Loss: 73.53201293945312\n",
      "Iteration no: 1541 and Loss: 73.51382446289062\n",
      "Iteration no: 1542 and Loss: 73.49568939208984\n",
      "Iteration no: 1543 and Loss: 73.47759246826172\n",
      "Iteration no: 1544 and Loss: 73.45952606201172\n",
      "Iteration no: 1545 and Loss: 73.4415054321289\n",
      "Iteration no: 1546 and Loss: 73.42353057861328\n",
      "Iteration no: 1547 and Loss: 73.40560913085938\n",
      "Iteration no: 1548 and Loss: 73.38771057128906\n",
      "Iteration no: 1549 and Loss: 73.36982727050781\n",
      "Iteration no: 1550 and Loss: 73.35202026367188\n",
      "Iteration no: 1551 and Loss: 73.33422088623047\n",
      "Iteration no: 1552 and Loss: 73.3165054321289\n",
      "Iteration no: 1553 and Loss: 73.29878997802734\n",
      "Iteration no: 1554 and Loss: 73.28112030029297\n",
      "Iteration no: 1555 and Loss: 73.26348114013672\n",
      "Iteration no: 1556 and Loss: 73.24588775634766\n",
      "Iteration no: 1557 and Loss: 73.22833251953125\n",
      "Iteration no: 1558 and Loss: 73.21082305908203\n",
      "Iteration no: 1559 and Loss: 73.19334411621094\n",
      "Iteration no: 1560 and Loss: 73.17594146728516\n",
      "Iteration no: 1561 and Loss: 73.1585464477539\n",
      "Iteration no: 1562 and Loss: 73.14117431640625\n",
      "Iteration no: 1563 and Loss: 73.12386322021484\n",
      "Iteration no: 1564 and Loss: 73.10657501220703\n",
      "Iteration no: 1565 and Loss: 73.08932495117188\n",
      "Iteration no: 1566 and Loss: 73.0721206665039\n",
      "Iteration no: 1567 and Loss: 73.0549545288086\n",
      "Iteration no: 1568 and Loss: 73.03783416748047\n",
      "Iteration no: 1569 and Loss: 73.02074432373047\n",
      "Iteration no: 1570 and Loss: 73.00367736816406\n",
      "Iteration no: 1571 and Loss: 72.98668670654297\n",
      "Iteration no: 1572 and Loss: 72.96971130371094\n",
      "Iteration no: 1573 and Loss: 72.95277404785156\n",
      "Iteration no: 1574 and Loss: 72.93592071533203\n",
      "Iteration no: 1575 and Loss: 72.91905975341797\n",
      "Iteration no: 1576 and Loss: 72.9022216796875\n",
      "Iteration no: 1577 and Loss: 72.88542938232422\n",
      "Iteration no: 1578 and Loss: 72.86869049072266\n",
      "Iteration no: 1579 and Loss: 72.85198211669922\n",
      "Iteration no: 1580 and Loss: 72.8353042602539\n",
      "Iteration no: 1581 and Loss: 72.81866455078125\n",
      "Iteration no: 1582 and Loss: 72.80207061767578\n",
      "Iteration no: 1583 and Loss: 72.78553009033203\n",
      "Iteration no: 1584 and Loss: 72.76899719238281\n",
      "Iteration no: 1585 and Loss: 72.75251770019531\n",
      "Iteration no: 1586 and Loss: 72.73606872558594\n",
      "Iteration no: 1587 and Loss: 72.71965026855469\n",
      "Iteration no: 1588 and Loss: 72.70330047607422\n",
      "Iteration no: 1589 and Loss: 72.68695831298828\n",
      "Iteration no: 1590 and Loss: 72.67064666748047\n",
      "Iteration no: 1591 and Loss: 72.65438842773438\n",
      "Iteration no: 1592 and Loss: 72.63817596435547\n",
      "Iteration no: 1593 and Loss: 72.62198638916016\n",
      "Iteration no: 1594 and Loss: 72.60584259033203\n",
      "Iteration no: 1595 and Loss: 72.5897445678711\n",
      "Iteration no: 1596 and Loss: 72.57363891601562\n",
      "Iteration no: 1597 and Loss: 72.55760955810547\n",
      "Iteration no: 1598 and Loss: 72.54161071777344\n",
      "Iteration no: 1599 and Loss: 72.52564239501953\n",
      "Iteration no: 1600 and Loss: 72.50969696044922\n",
      "Iteration no: 1601 and Loss: 72.49382019042969\n",
      "Iteration no: 1602 and Loss: 72.47795867919922\n",
      "Iteration no: 1603 and Loss: 72.46212768554688\n",
      "Iteration no: 1604 and Loss: 72.44634246826172\n",
      "Iteration no: 1605 and Loss: 72.43060302734375\n",
      "Iteration no: 1606 and Loss: 72.41488647460938\n",
      "Iteration no: 1607 and Loss: 72.39923095703125\n",
      "Iteration no: 1608 and Loss: 72.38360595703125\n",
      "Iteration no: 1609 and Loss: 72.36797332763672\n",
      "Iteration no: 1610 and Loss: 72.3524169921875\n",
      "Iteration no: 1611 and Loss: 72.33687591552734\n",
      "Iteration no: 1612 and Loss: 72.32141876220703\n",
      "Iteration no: 1613 and Loss: 72.30592346191406\n",
      "Iteration no: 1614 and Loss: 72.29051971435547\n",
      "Iteration no: 1615 and Loss: 72.27513122558594\n",
      "Iteration no: 1616 and Loss: 72.25979614257812\n",
      "Iteration no: 1617 and Loss: 72.24446868896484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration no: 1618 and Loss: 72.22918701171875\n",
      "Iteration no: 1619 and Loss: 72.21393585205078\n",
      "Iteration no: 1620 and Loss: 72.19873809814453\n",
      "Iteration no: 1621 and Loss: 72.18354034423828\n",
      "Iteration no: 1622 and Loss: 72.16841888427734\n",
      "Iteration no: 1623 and Loss: 72.15328979492188\n",
      "Iteration no: 1624 and Loss: 72.13824462890625\n",
      "Iteration no: 1625 and Loss: 72.1231918334961\n",
      "Iteration no: 1626 and Loss: 72.10821533203125\n",
      "Iteration no: 1627 and Loss: 72.09323120117188\n",
      "Iteration no: 1628 and Loss: 72.07831573486328\n",
      "Iteration no: 1629 and Loss: 72.06341552734375\n",
      "Iteration no: 1630 and Loss: 72.04855346679688\n",
      "Iteration no: 1631 and Loss: 72.03372955322266\n",
      "Iteration no: 1632 and Loss: 72.0189437866211\n",
      "Iteration no: 1633 and Loss: 72.00416564941406\n",
      "Iteration no: 1634 and Loss: 71.98945617675781\n",
      "Iteration no: 1635 and Loss: 71.97473907470703\n",
      "Iteration no: 1636 and Loss: 71.9601058959961\n",
      "Iteration no: 1637 and Loss: 71.94548797607422\n",
      "Iteration no: 1638 and Loss: 71.9308853149414\n",
      "Iteration no: 1639 and Loss: 71.91636657714844\n",
      "Iteration no: 1640 and Loss: 71.90182495117188\n",
      "Iteration no: 1641 and Loss: 71.88735961914062\n",
      "Iteration no: 1642 and Loss: 71.87291717529297\n",
      "Iteration no: 1643 and Loss: 71.85846710205078\n",
      "Iteration no: 1644 and Loss: 71.84408569335938\n",
      "Iteration no: 1645 and Loss: 71.8297348022461\n",
      "Iteration no: 1646 and Loss: 71.81536102294922\n",
      "Iteration no: 1647 and Loss: 71.80108642578125\n",
      "Iteration no: 1648 and Loss: 71.78681945800781\n",
      "Iteration no: 1649 and Loss: 71.77254486083984\n",
      "Iteration no: 1650 and Loss: 71.75836944580078\n",
      "Iteration no: 1651 and Loss: 71.7441635131836\n",
      "Iteration no: 1652 and Loss: 71.73005676269531\n",
      "Iteration no: 1653 and Loss: 71.7159423828125\n",
      "Iteration no: 1654 and Loss: 71.70185089111328\n",
      "Iteration no: 1655 and Loss: 71.68782806396484\n",
      "Iteration no: 1656 and Loss: 71.67382049560547\n",
      "Iteration no: 1657 and Loss: 71.65984344482422\n",
      "Iteration no: 1658 and Loss: 71.6458969116211\n",
      "Iteration no: 1659 and Loss: 71.63200378417969\n",
      "Iteration no: 1660 and Loss: 71.61811065673828\n",
      "Iteration no: 1661 and Loss: 71.60424041748047\n",
      "Iteration no: 1662 and Loss: 71.5904541015625\n",
      "Iteration no: 1663 and Loss: 71.57664489746094\n",
      "Iteration no: 1664 and Loss: 71.56291961669922\n",
      "Iteration no: 1665 and Loss: 71.5492172241211\n",
      "Iteration no: 1666 and Loss: 71.53550720214844\n",
      "Iteration no: 1667 and Loss: 71.5218734741211\n",
      "Iteration no: 1668 and Loss: 71.50827026367188\n",
      "Iteration no: 1669 and Loss: 71.49468231201172\n",
      "Iteration no: 1670 and Loss: 71.4811019897461\n",
      "Iteration no: 1671 and Loss: 71.46759033203125\n",
      "Iteration no: 1672 and Loss: 71.45408630371094\n",
      "Iteration no: 1673 and Loss: 71.44064331054688\n",
      "Iteration no: 1674 and Loss: 71.42719268798828\n",
      "Iteration no: 1675 and Loss: 71.41381072998047\n",
      "Iteration no: 1676 and Loss: 71.40045928955078\n",
      "Iteration no: 1677 and Loss: 71.38712310791016\n",
      "Iteration no: 1678 and Loss: 71.37384033203125\n",
      "Iteration no: 1679 and Loss: 71.3605728149414\n",
      "Iteration no: 1680 and Loss: 71.3473129272461\n",
      "Iteration no: 1681 and Loss: 71.33413696289062\n",
      "Iteration no: 1682 and Loss: 71.32099151611328\n",
      "Iteration no: 1683 and Loss: 71.30785369873047\n",
      "Iteration no: 1684 and Loss: 71.29479217529297\n",
      "Iteration no: 1685 and Loss: 71.2817611694336\n",
      "Iteration no: 1686 and Loss: 71.26875305175781\n",
      "Iteration no: 1687 and Loss: 71.25576782226562\n",
      "Iteration no: 1688 and Loss: 71.24283599853516\n",
      "Iteration no: 1689 and Loss: 71.22989654541016\n",
      "Iteration no: 1690 and Loss: 71.2170181274414\n",
      "Iteration no: 1691 and Loss: 71.20417785644531\n",
      "Iteration no: 1692 and Loss: 71.19136810302734\n",
      "Iteration no: 1693 and Loss: 71.17855072021484\n",
      "Iteration no: 1694 and Loss: 71.16581726074219\n",
      "Iteration no: 1695 and Loss: 71.15308380126953\n",
      "Iteration no: 1696 and Loss: 71.14026641845703\n",
      "Iteration no: 1697 and Loss: 71.12755584716797\n",
      "Iteration no: 1698 and Loss: 71.11483764648438\n",
      "Iteration no: 1699 and Loss: 71.10213470458984\n",
      "Iteration no: 1700 and Loss: 71.08944702148438\n",
      "Iteration no: 1701 and Loss: 71.07679748535156\n",
      "Iteration no: 1702 and Loss: 71.06420135498047\n",
      "Iteration no: 1703 and Loss: 71.05159759521484\n",
      "Iteration no: 1704 and Loss: 71.03900909423828\n",
      "Iteration no: 1705 and Loss: 71.02648162841797\n",
      "Iteration no: 1706 and Loss: 71.01394653320312\n",
      "Iteration no: 1707 and Loss: 71.0014877319336\n",
      "Iteration no: 1708 and Loss: 70.9890365600586\n",
      "Iteration no: 1709 and Loss: 70.97659301757812\n",
      "Iteration no: 1710 and Loss: 70.96421813964844\n",
      "Iteration no: 1711 and Loss: 70.95182037353516\n",
      "Iteration no: 1712 and Loss: 70.93948364257812\n",
      "Iteration no: 1713 and Loss: 70.92716979980469\n",
      "Iteration no: 1714 and Loss: 70.91487884521484\n",
      "Iteration no: 1715 and Loss: 70.9026107788086\n",
      "Iteration no: 1716 and Loss: 70.89036560058594\n",
      "Iteration no: 1717 and Loss: 70.87818145751953\n",
      "Iteration no: 1718 and Loss: 70.86601257324219\n",
      "Iteration no: 1719 and Loss: 70.85387420654297\n",
      "Iteration no: 1720 and Loss: 70.841796875\n",
      "Iteration no: 1721 and Loss: 70.82969665527344\n",
      "Iteration no: 1722 and Loss: 70.81764221191406\n",
      "Iteration no: 1723 and Loss: 70.80561065673828\n",
      "Iteration no: 1724 and Loss: 70.79364776611328\n",
      "Iteration no: 1725 and Loss: 70.78170013427734\n",
      "Iteration no: 1726 and Loss: 70.76966094970703\n",
      "Iteration no: 1727 and Loss: 70.75767517089844\n",
      "Iteration no: 1728 and Loss: 70.74569702148438\n",
      "Iteration no: 1729 and Loss: 70.7337417602539\n",
      "Iteration no: 1730 and Loss: 70.72181701660156\n",
      "Iteration no: 1731 and Loss: 70.70989990234375\n",
      "Iteration no: 1732 and Loss: 70.69801330566406\n",
      "Iteration no: 1733 and Loss: 70.68614959716797\n",
      "Iteration no: 1734 and Loss: 70.67430877685547\n",
      "Iteration no: 1735 and Loss: 70.66252899169922\n",
      "Iteration no: 1736 and Loss: 70.65074157714844\n",
      "Iteration no: 1737 and Loss: 70.63896942138672\n",
      "Iteration no: 1738 and Loss: 70.6272201538086\n",
      "Iteration no: 1739 and Loss: 70.61551666259766\n",
      "Iteration no: 1740 and Loss: 70.60385131835938\n",
      "Iteration no: 1741 and Loss: 70.59219360351562\n",
      "Iteration no: 1742 and Loss: 70.58057403564453\n",
      "Iteration no: 1743 and Loss: 70.56898498535156\n",
      "Iteration no: 1744 and Loss: 70.55741882324219\n",
      "Iteration no: 1745 and Loss: 70.54588317871094\n",
      "Iteration no: 1746 and Loss: 70.53437805175781\n",
      "Iteration no: 1747 and Loss: 70.52288818359375\n",
      "Iteration no: 1748 and Loss: 70.51143646240234\n",
      "Iteration no: 1749 and Loss: 70.49999237060547\n",
      "Iteration no: 1750 and Loss: 70.48856353759766\n",
      "Iteration no: 1751 and Loss: 70.47714233398438\n",
      "Iteration no: 1752 and Loss: 70.46574401855469\n",
      "Iteration no: 1753 and Loss: 70.45435333251953\n",
      "Iteration no: 1754 and Loss: 70.4429702758789\n",
      "Iteration no: 1755 and Loss: 70.4316177368164\n",
      "Iteration no: 1756 and Loss: 70.4202880859375\n",
      "Iteration no: 1757 and Loss: 70.40901184082031\n",
      "Iteration no: 1758 and Loss: 70.39778137207031\n",
      "Iteration no: 1759 and Loss: 70.3864974975586\n",
      "Iteration no: 1760 and Loss: 70.37529754638672\n",
      "Iteration no: 1761 and Loss: 70.3641128540039\n",
      "Iteration no: 1762 and Loss: 70.35294342041016\n",
      "Iteration no: 1763 and Loss: 70.34178161621094\n",
      "Iteration no: 1764 and Loss: 70.33070373535156\n",
      "Iteration no: 1765 and Loss: 70.31964874267578\n",
      "Iteration no: 1766 and Loss: 70.30858612060547\n",
      "Iteration no: 1767 and Loss: 70.29755401611328\n",
      "Iteration no: 1768 and Loss: 70.28657531738281\n",
      "Iteration no: 1769 and Loss: 70.27560424804688\n",
      "Iteration no: 1770 and Loss: 70.2646484375\n",
      "Iteration no: 1771 and Loss: 70.2537612915039\n",
      "Iteration no: 1772 and Loss: 70.24288177490234\n",
      "Iteration no: 1773 and Loss: 70.23202514648438\n",
      "Iteration no: 1774 and Loss: 70.22120666503906\n",
      "Iteration no: 1775 and Loss: 70.21044158935547\n",
      "Iteration no: 1776 and Loss: 70.19966888427734\n",
      "Iteration no: 1777 and Loss: 70.1889419555664\n",
      "Iteration no: 1778 and Loss: 70.17821502685547\n",
      "Iteration no: 1779 and Loss: 70.16756439208984\n",
      "Iteration no: 1780 and Loss: 70.15687561035156\n",
      "Iteration no: 1781 and Loss: 70.14627838134766\n",
      "Iteration no: 1782 and Loss: 70.13570404052734\n",
      "Iteration no: 1783 and Loss: 70.12513732910156\n",
      "Iteration no: 1784 and Loss: 70.11458587646484\n",
      "Iteration no: 1785 and Loss: 70.10405731201172\n",
      "Iteration no: 1786 and Loss: 70.09357452392578\n",
      "Iteration no: 1787 and Loss: 70.0831298828125\n",
      "Iteration no: 1788 and Loss: 70.07269287109375\n",
      "Iteration no: 1789 and Loss: 70.06229400634766\n",
      "Iteration no: 1790 and Loss: 70.05193328857422\n",
      "Iteration no: 1791 and Loss: 70.04158020019531\n",
      "Iteration no: 1792 and Loss: 70.0312728881836\n",
      "Iteration no: 1793 and Loss: 70.02096557617188\n",
      "Iteration no: 1794 and Loss: 70.01068878173828\n",
      "Iteration no: 1795 and Loss: 70.00045013427734\n",
      "Iteration no: 1796 and Loss: 69.99022674560547\n",
      "Iteration no: 1797 and Loss: 69.98003387451172\n",
      "Iteration no: 1798 and Loss: 69.96987915039062\n",
      "Iteration no: 1799 and Loss: 69.95970916748047\n",
      "Iteration no: 1800 and Loss: 69.94960021972656\n",
      "Iteration no: 1801 and Loss: 69.939453125\n",
      "Iteration no: 1802 and Loss: 69.92936706542969\n",
      "Iteration no: 1803 and Loss: 69.91931915283203\n",
      "Iteration no: 1804 and Loss: 69.90927124023438\n",
      "Iteration no: 1805 and Loss: 69.89924621582031\n",
      "Iteration no: 1806 and Loss: 69.88923645019531\n",
      "Iteration no: 1807 and Loss: 69.87931823730469\n",
      "Iteration no: 1808 and Loss: 69.86934661865234\n",
      "Iteration no: 1809 and Loss: 69.85946655273438\n",
      "Iteration no: 1810 and Loss: 69.84954833984375\n",
      "Iteration no: 1811 and Loss: 69.8396987915039\n",
      "Iteration no: 1812 and Loss: 69.82987976074219\n",
      "Iteration no: 1813 and Loss: 69.82003784179688\n",
      "Iteration no: 1814 and Loss: 69.81024169921875\n",
      "Iteration no: 1815 and Loss: 69.80049133300781\n",
      "Iteration no: 1816 and Loss: 69.79074096679688\n",
      "Iteration no: 1817 and Loss: 69.78103637695312\n",
      "Iteration no: 1818 and Loss: 69.7713394165039\n",
      "Iteration no: 1819 and Loss: 69.7616958618164\n",
      "Iteration no: 1820 and Loss: 69.75204467773438\n",
      "Iteration no: 1821 and Loss: 69.74241638183594\n",
      "Iteration no: 1822 and Loss: 69.73283386230469\n",
      "Iteration no: 1823 and Loss: 69.7232894897461\n",
      "Iteration no: 1824 and Loss: 69.7137680053711\n",
      "Iteration no: 1825 and Loss: 69.70423126220703\n",
      "Iteration no: 1826 and Loss: 69.69474029541016\n",
      "Iteration no: 1827 and Loss: 69.68527221679688\n",
      "Iteration no: 1828 and Loss: 69.67582702636719\n",
      "Iteration no: 1829 and Loss: 69.66641998291016\n",
      "Iteration no: 1830 and Loss: 69.65702819824219\n",
      "Iteration no: 1831 and Loss: 69.64765930175781\n",
      "Iteration no: 1832 and Loss: 69.63832092285156\n",
      "Iteration no: 1833 and Loss: 69.62897491455078\n",
      "Iteration no: 1834 and Loss: 69.61971282958984\n",
      "Iteration no: 1835 and Loss: 69.61043548583984\n",
      "Iteration no: 1836 and Loss: 69.60120391845703\n",
      "Iteration no: 1837 and Loss: 69.59197998046875\n",
      "Iteration no: 1838 and Loss: 69.58280944824219\n",
      "Iteration no: 1839 and Loss: 69.57367706298828\n",
      "Iteration no: 1840 and Loss: 69.56454467773438\n",
      "Iteration no: 1841 and Loss: 69.5554428100586\n",
      "Iteration no: 1842 and Loss: 69.5463638305664\n",
      "Iteration no: 1843 and Loss: 69.5373306274414\n",
      "Iteration no: 1844 and Loss: 69.52828979492188\n",
      "Iteration no: 1845 and Loss: 69.51929473876953\n",
      "Iteration no: 1846 and Loss: 69.51032257080078\n",
      "Iteration no: 1847 and Loss: 69.50135040283203\n",
      "Iteration no: 1848 and Loss: 69.49238586425781\n",
      "Iteration no: 1849 and Loss: 69.48348236083984\n",
      "Iteration no: 1850 and Loss: 69.4746322631836\n",
      "Iteration no: 1851 and Loss: 69.46576690673828\n",
      "Iteration no: 1852 and Loss: 69.4569091796875\n",
      "Iteration no: 1853 and Loss: 69.44811248779297\n",
      "Iteration no: 1854 and Loss: 69.43932342529297\n",
      "Iteration no: 1855 and Loss: 69.43055725097656\n",
      "Iteration no: 1856 and Loss: 69.42181396484375\n",
      "Iteration no: 1857 and Loss: 69.4130859375\n",
      "Iteration no: 1858 and Loss: 69.40440368652344\n",
      "Iteration no: 1859 and Loss: 69.39570617675781\n",
      "Iteration no: 1860 and Loss: 69.38706970214844\n",
      "Iteration no: 1861 and Loss: 69.37845611572266\n",
      "Iteration no: 1862 and Loss: 69.36980438232422\n",
      "Iteration no: 1863 and Loss: 69.36122131347656\n",
      "Iteration no: 1864 and Loss: 69.3526382446289\n",
      "Iteration no: 1865 and Loss: 69.34410095214844\n",
      "Iteration no: 1866 and Loss: 69.33555603027344\n",
      "Iteration no: 1867 and Loss: 69.32706451416016\n",
      "Iteration no: 1868 and Loss: 69.31857299804688\n",
      "Iteration no: 1869 and Loss: 69.31011199951172\n",
      "Iteration no: 1870 and Loss: 69.3016586303711\n",
      "Iteration no: 1871 and Loss: 69.29324340820312\n",
      "Iteration no: 1872 and Loss: 69.28482055664062\n",
      "Iteration no: 1873 and Loss: 69.27645874023438\n",
      "Iteration no: 1874 and Loss: 69.26810455322266\n",
      "Iteration no: 1875 and Loss: 69.25975036621094\n",
      "Iteration no: 1876 and Loss: 69.25144958496094\n",
      "Iteration no: 1877 and Loss: 69.24315643310547\n",
      "Iteration no: 1878 and Loss: 69.23490142822266\n",
      "Iteration no: 1879 and Loss: 69.2266616821289\n",
      "Iteration no: 1880 and Loss: 69.2184066772461\n",
      "Iteration no: 1881 and Loss: 69.21019744873047\n",
      "Iteration no: 1882 and Loss: 69.20201110839844\n",
      "Iteration no: 1883 and Loss: 69.19387817382812\n",
      "Iteration no: 1884 and Loss: 69.18570709228516\n",
      "Iteration no: 1885 and Loss: 69.1775894165039\n",
      "Iteration no: 1886 and Loss: 69.16950988769531\n",
      "Iteration no: 1887 and Loss: 69.16142272949219\n",
      "Iteration no: 1888 and Loss: 69.15337371826172\n",
      "Iteration no: 1889 and Loss: 69.14533996582031\n",
      "Iteration no: 1890 and Loss: 69.13734436035156\n",
      "Iteration no: 1891 and Loss: 69.12934112548828\n",
      "Iteration no: 1892 and Loss: 69.12138366699219\n",
      "Iteration no: 1893 and Loss: 69.11343383789062\n",
      "Iteration no: 1894 and Loss: 69.10547637939453\n",
      "Iteration no: 1895 and Loss: 69.09757995605469\n",
      "Iteration no: 1896 and Loss: 69.08969116210938\n",
      "Iteration no: 1897 and Loss: 69.08184814453125\n",
      "Iteration no: 1898 and Loss: 69.0739974975586\n",
      "Iteration no: 1899 and Loss: 69.06619262695312\n",
      "Iteration no: 1900 and Loss: 69.0583724975586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration no: 1901 and Loss: 69.05058288574219\n",
      "Iteration no: 1902 and Loss: 69.0428237915039\n",
      "Iteration no: 1903 and Loss: 69.0350570678711\n",
      "Iteration no: 1904 and Loss: 69.0273666381836\n",
      "Iteration no: 1905 and Loss: 69.0196762084961\n",
      "Iteration no: 1906 and Loss: 69.0121078491211\n",
      "Iteration no: 1907 and Loss: 69.00460052490234\n",
      "Iteration no: 1908 and Loss: 68.9970932006836\n",
      "Iteration no: 1909 and Loss: 68.9896469116211\n",
      "Iteration no: 1910 and Loss: 68.9822006225586\n",
      "Iteration no: 1911 and Loss: 68.97476959228516\n",
      "Iteration no: 1912 and Loss: 68.9673843383789\n",
      "Iteration no: 1913 and Loss: 68.96000671386719\n",
      "Iteration no: 1914 and Loss: 68.95263671875\n",
      "Iteration no: 1915 and Loss: 68.94534301757812\n",
      "Iteration no: 1916 and Loss: 68.93802642822266\n",
      "Iteration no: 1917 and Loss: 68.93070220947266\n",
      "Iteration no: 1918 and Loss: 68.9234619140625\n",
      "Iteration no: 1919 and Loss: 68.91621398925781\n",
      "Iteration no: 1920 and Loss: 68.90898132324219\n",
      "Iteration no: 1921 and Loss: 68.90177917480469\n",
      "Iteration no: 1922 and Loss: 68.89459228515625\n",
      "Iteration no: 1923 and Loss: 68.8874282836914\n",
      "Iteration no: 1924 and Loss: 68.88027954101562\n",
      "Iteration no: 1925 and Loss: 68.87315368652344\n",
      "Iteration no: 1926 and Loss: 68.86605072021484\n",
      "Iteration no: 1927 and Loss: 68.85896301269531\n",
      "Iteration no: 1928 and Loss: 68.8519058227539\n",
      "Iteration no: 1929 and Loss: 68.84485626220703\n",
      "Iteration no: 1930 and Loss: 68.83780670166016\n",
      "Iteration no: 1931 and Loss: 68.830810546875\n",
      "Iteration no: 1932 and Loss: 68.8238296508789\n",
      "Iteration no: 1933 and Loss: 68.8168716430664\n",
      "Iteration no: 1934 and Loss: 68.80989837646484\n",
      "Iteration no: 1935 and Loss: 68.80294799804688\n",
      "Iteration no: 1936 and Loss: 68.79605865478516\n",
      "Iteration no: 1937 and Loss: 68.7891616821289\n",
      "Iteration no: 1938 and Loss: 68.78226470947266\n",
      "Iteration no: 1939 and Loss: 68.77544403076172\n",
      "Iteration no: 1940 and Loss: 68.76859283447266\n",
      "Iteration no: 1941 and Loss: 68.7617416381836\n",
      "Iteration no: 1942 and Loss: 68.75495147705078\n",
      "Iteration no: 1943 and Loss: 68.74819946289062\n",
      "Iteration no: 1944 and Loss: 68.74141693115234\n",
      "Iteration no: 1945 and Loss: 68.73466491699219\n",
      "Iteration no: 1946 and Loss: 68.72795104980469\n",
      "Iteration no: 1947 and Loss: 68.72123718261719\n",
      "Iteration no: 1948 and Loss: 68.71455383300781\n",
      "Iteration no: 1949 and Loss: 68.70787811279297\n",
      "Iteration no: 1950 and Loss: 68.70122528076172\n",
      "Iteration no: 1951 and Loss: 68.69458770751953\n",
      "Iteration no: 1952 and Loss: 68.6879653930664\n",
      "Iteration no: 1953 and Loss: 68.6813735961914\n",
      "Iteration no: 1954 and Loss: 68.6747817993164\n",
      "Iteration no: 1955 and Loss: 68.66822052001953\n",
      "Iteration no: 1956 and Loss: 68.66169738769531\n",
      "Iteration no: 1957 and Loss: 68.6551513671875\n",
      "Iteration no: 1958 and Loss: 68.64863586425781\n",
      "Iteration no: 1959 and Loss: 68.64213562011719\n",
      "Iteration no: 1960 and Loss: 68.63567352294922\n",
      "Iteration no: 1961 and Loss: 68.62921905517578\n",
      "Iteration no: 1962 and Loss: 68.62278747558594\n",
      "Iteration no: 1963 and Loss: 68.61634063720703\n",
      "Iteration no: 1964 and Loss: 68.60995483398438\n",
      "Iteration no: 1965 and Loss: 68.60357666015625\n",
      "Iteration no: 1966 and Loss: 68.59718322753906\n",
      "Iteration no: 1967 and Loss: 68.59085083007812\n",
      "Iteration no: 1968 and Loss: 68.58450317382812\n",
      "Iteration no: 1969 and Loss: 68.57820129394531\n",
      "Iteration no: 1970 and Loss: 68.57189178466797\n",
      "Iteration no: 1971 and Loss: 68.56562805175781\n",
      "Iteration no: 1972 and Loss: 68.5593490600586\n",
      "Iteration no: 1973 and Loss: 68.5531005859375\n",
      "Iteration no: 1974 and Loss: 68.54686737060547\n",
      "Iteration no: 1975 and Loss: 68.54065704345703\n",
      "Iteration no: 1976 and Loss: 68.5344467163086\n",
      "Iteration no: 1977 and Loss: 68.52826690673828\n",
      "Iteration no: 1978 and Loss: 68.5221176147461\n",
      "Iteration no: 1979 and Loss: 68.51595306396484\n",
      "Iteration no: 1980 and Loss: 68.50983428955078\n",
      "Iteration no: 1981 and Loss: 68.50371551513672\n",
      "Iteration no: 1982 and Loss: 68.49762725830078\n",
      "Iteration no: 1983 and Loss: 68.49156188964844\n",
      "Iteration no: 1984 and Loss: 68.48548889160156\n",
      "Iteration no: 1985 and Loss: 68.47944641113281\n",
      "Iteration no: 1986 and Loss: 68.4734115600586\n",
      "Iteration no: 1987 and Loss: 68.46736907958984\n",
      "Iteration no: 1988 and Loss: 68.46138763427734\n",
      "Iteration no: 1989 and Loss: 68.45541381835938\n",
      "Iteration no: 1990 and Loss: 68.44944763183594\n",
      "Iteration no: 1991 and Loss: 68.44351959228516\n",
      "Iteration no: 1992 and Loss: 68.43759155273438\n",
      "Iteration no: 1993 and Loss: 68.43165588378906\n",
      "Iteration no: 1994 and Loss: 68.42577362060547\n",
      "Iteration no: 1995 and Loss: 68.41989135742188\n",
      "Iteration no: 1996 and Loss: 68.41400146484375\n",
      "Iteration no: 1997 and Loss: 68.40815734863281\n",
      "Iteration no: 1998 and Loss: 68.4023208618164\n",
      "Iteration no: 1999 and Loss: 68.3965072631836\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(2000):\n",
    "    \n",
    "    y_pred = model(x_train_tensor)\n",
    "    \n",
    "    loss = loss_func(y_pred, y_train_tensor)\n",
    "    print('Iteration no: %s and Loss: %s' %(i, loss.item()))\n",
    "                             \n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "                             \n",
    "    optimizer.step()\n",
    "                                                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9473],\n",
       "        [ 0.2440],\n",
       "        [-0.1898],\n",
       "        [ 0.4537],\n",
       "        [ 0.0265],\n",
       "        [ 0.9229],\n",
       "        [ 0.5890],\n",
       "        [ 0.7367],\n",
       "        [ 0.1238],\n",
       "        [-0.9014]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_tensor = model(x_test_tensor)\n",
    "    \n",
    "y_pred_tensor[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.94728816],\n",
       "       [ 0.24395555],\n",
       "       [-0.18977496],\n",
       "       [ 0.45367888],\n",
       "       [ 0.02645588],\n",
       "       [ 0.92290306],\n",
       "       [ 0.58900094],\n",
       "       [ 0.7367346 ],\n",
       "       [ 0.12378177],\n",
       "       [-0.90138304]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = y_pred_tensor.detach().numpy()\n",
    "\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.scatter(x_test, y_test, s=200, c='pink', label='Actual value')\n",
    "plt.plot(x_test, y_pred, label='Predicted value', c='darkgreen')\n",
    "\n",
    "plt.xlabel('Head Size', fontsize=20)\n",
    "plt.ylabel('Brain Weight', fontsize=20)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-114.07078545479531"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
